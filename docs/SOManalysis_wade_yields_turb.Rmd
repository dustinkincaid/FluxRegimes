---
title: <font size = "5"> Self-organizing map analysis for event turbidity yields at Wade
  </font>
author: "Dustin Kincaid"
date: "04/13/2021<br><br>"
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r notes, include=FALSE}
# A helpful website about Rmarkdown
  # https://ourcodingclub.github.io/tutorials/rmarkdown/

# Can't have a lattice configuration with more nodes than observations in the analysis
```

#### Objective  
Cluster high-flow event characteristics and antecedent watershed conditions to evaluate how these factors converge as flux regimes (clusters) to produce variability in event turbidity yields  
<br><br>

```{r setup, include=FALSE, warning=FALSE, message = FALSE}
# Set defaults
knitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, out.width = "70%", fig.align = "center") 

# Load packages ----
  library("here")      # to make file paths fail less
  library("tidyverse") # general workhorse
  library("lubridate") # to work with dates
  library("kohonen")   # to run the SOM
  library("vegan")     # to run adonis() for calc of nonparametric F test
  # devtools::install_github("laresbernardo/lares")
  library("lares")     # examine var correlations
  library("ggforce")   # for facet plots on multiple pages
  library("gt")       # print pretty tables
  library("cowplot")  # plotting assistance
  library("patchwork")
  library("grid")     
  library("gridExtra")
  library("factoextra")  # for PCA plots

  # Load the range normalization function written by Kristin Underwood
  source(here("Scripts", "L2norm.R"))

# Create a new folder to save the plots and CSVs produced by the SOM code
newFolder <- paste("Data/somResults/wade/yields/turb/", Sys.Date(), sep = "")
dir.create(here(newFolder), recursive = TRUE)

# Give a site label to file names produced in SOM code
  myDataSet <- c("wade")

# Read in data
  # Calculated event metrics for each site as calculated in compile_calculate_allVars.R
  wade <- read_csv(here("Data", "eventMetrics_wade.csv")) %>% 
    mutate(event_start = ymd_hms(event_start, tz = "Etc/GMT+4")) %>% 
    # Code the multipeak col as numerical
    mutate(multipeak = ifelse(multipeak == "NO", 0, 1))
```

#### Select variables to keep
Per K Underwood: if two variables are strongly correlated (negatively or positively) they can effectively "double-weight" a particular factor important in driving clustering; thus, keep just one of the variables to serve as a proxy for that factor

```{r, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, out.width = "80%"}
# Select vars to keep Step 1
# List the variables that will not be used in the analysis (e.g., response/dependent & ID/INFO)
drop.info <- names(wade %>% 
                     select(# Remove the non-numerical ID/INFO
                            site, season, event_start,
                            # Remove response variables
                            NO3_kg_km2, SRP_kg_km2, event_NO3_SRP, turb_kg_km2, turb_event_max))

# Look at correlations between variables
wade %>% 
  # Remove the vars listed above
  select(-one_of(drop.info)) %>% 
  # Remove irrelevant variables
  # select(-c(multipeak)) %>% 
  # Remove all rain_Xd vars and keep the antecedent precipitation index (API) instead
  select(-c(rain_1d:rain_30d)) %>% 
  # Remove all GW event data (e.g., max and change); keep pre-event level data for now
  # Keeping GW level data limits the # of events we cluster b/c no GW data in 2017
  # Below I use a VWC variable as a proxy for pre-event GW levels and remove these
  select(-c(starts_with("gw_event_"))) %>% 
  # Remove all pre-event GW levels for individual wells and keep the aggregate ones
  select(-c(starts_with(c("gw_1d_well", "gw_4d_well")))) %>% 
  # Remove all pre-event soil var means and keep the aggregate "dry" and "wet" ones
  select(-c(contains("WWp"))) %>% 
  # Remove variables that don't likely matter to turbidity yields
  select(-c(starts_with("DO"), starts_with("Redox"))) %>% 
  # Let's use 15cm values for SoilTemp for now; remove 30cm & 45cm vars 
  select(-c(SoilTemp_pre_dry_30cm, SoilTemp_pre_wet_30cm,
            SoilTemp_pre_dry_45cm, SoilTemp_pre_wet_45cm)) %>%
  # For now, let's drop the aggreate soil vars for the dry site and keep the wet ones
  # select(-c(contains("pre_dry"))) %>% 
  # Keep only complete observations/rows (no NAs in any of the columns)
  na.omit() %>% 
  # Visualize these
  lares::corr_cross(max_pvalue = 0.05, top = 30)
```

```{r, include = FALSE}
# Look at plots of correlated variables
  # GW level ~ VWC
  wade %>%
    # pivot_longer(cols = c(VWC_pre_pit1_15cm, VWC_pre_pit1_30cm, VWC_pre_pit1_45cm, VWC_pre_pit3_15cm, VWC_pre_pit3_30cm, VWC_pre_pit3_45cm), names_to = "var", values_to = "level") %>%
    pivot_longer(cols = c(VWC_pre_wet_15cm, VWC_pre_wet_30cm), names_to = "var", values_to = "VWC") %>%
    ggplot(aes(x = gw_1d_allWells, y = VWC, color = var)) +
    geom_point(shape = 1) +
    geom_abline(slope = 1) +
    theme_classic()

  # Other correlations
  # scatter plot comparison
  wade %>% 
    ggplot(aes(x = gw_1d_allWells, y = gw_4d_allWells)) +
    geom_point(shape = 1) +
    geom_abline(slope = 1) +
    geom_smooth(method = "lm") +
    theme_classic()
  # boxplot comparison
  wade %>% 
    pivot_longer(cols = c(rain_int_mmPERmin_mean, rain_int_mmPERmin_max), names_to = "var", values_to = "value") %>% 
    ggplot(aes(x = var, y = value)) +
    geom_boxplot()
  
  wade %>% 
    ggplot(aes(x = rain_int_mmPERmin_max)) +
    geom_histogram()
```

##### Decisions for eliminating variables w/ correlations >70%

* **These decisions were tough to make and need review**

* **I feel OK about these decisions, but they should be reviewed as well**
* If the 1-d and 4-d values for a variable are highly correlated, use the 4-d value
  + airT_1d and airT_4d are highly correlated (92.5%); removing airT_1d
  + EXCEPTION: gw_1d_allWells and gw_4d_allWells are highly correlated (91.3%); BUT the 1d value is better correlated w/ a VWC variable (gw_1d + VWC_pre_wet_30cm = 91.5% vs. gw_4d + VWC_pre_wet_30cm = 84.8%); so remove gw_4d_allWells
  + q_1d and q_4d highly correlated (89.5%); removing q_1d
  + solarRad_1d and solarRad_4d are positively correlated (70.8%); drop solarRad_1d per rule above  
* Trying to find a VWC variable that correlates well with GW level so that I can remove GW level vars (no GW data in 2017)
  + gw_1d_allWells and VWC_pre_wet_30cm most highly correlated (91.5%); same with VWC_pre_wet_15cm (85.3%); keep just one b/c the two VWC's are correlated 95.3%; keep VWC_pre_wet_30cm
  + drop gw_1d_allWells and use VWC as a proxy for GW level
* Soil variables
  + SoilTemp_pre_dry_15cm & equivalent at wet transect are highly correlated (99.7%); let's keep wet transect temps
  + drop all 30cm and 45cm soil vars except for when 30cm makes sense to keep (eg. VWC_pre_wet_30cm as proxy for GW level)  
  + VWC_pre_dry_15cm and VWC_pre_wet_30cm highly correlated (77.5%); drop VWC_pre_dry_15cm to preserve the other as proxy for GW level
  + DO_pre_wet_15cm + Redox_pre_wet_15cm are correlated (69.3%) and are on the same PCA vector; I should remove one; I removed redox for NO3 SOM so remove redox here
* MET variables
  + airT_4d and dewPoint_4d are highly correlated (96.2%), as is dewPoint_1d (93%); removing both dewPoints
  + airT_4d and SoilTemp_pre_wet_15cm are highly correlated (90.7%); drop airT_4d b/c we still have diff_airT_soilT
  + rain_int_mmPERmin_max + rain_int_mmPERmin_mean aren't super correlated (42.4%), but represent similar things, so removing the mean version  
* Stream variables 
  + q_event_delta & q_event_max are highly correlated (88.2%); let's keep q_event_max  
  + q_event_max + q_mm are highly correlated (74.2%); let's keep q_event_max b/c NO3 & SRP yields calculated using q_mm
  + q_event_max + turb_event_max are correlated (71.5%); keep q_event_max 
* **I removed these variables after the initial SOM run b/c they did not differentiate clusters**
<br><br>

##### Look at correlations again after dropping variables
```{r, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, out.width = "80%"}
# Look at correlations again without dropped variables
  # List the additional dropped variables
  drop.vars <- names(wade %>% 
                       select(# Remove multipeak for now b/c we have q_4d and time_sinceLastEvent
                              multipeak,
                              # Remove all rain_Xd vars and keep API instead
                              rain_1d:rain_30d,
                              # Remove all GW event data (e.g., max and change); keep pre-event level data for now
                              # Keeping GW level data limits the # of events we cluster b/c no GW data in 2017
                              # Below I use a VWC variable as a proxy for pre-event GW levels and remove these
                              starts_with("gw_event_"),
                              # Remove all pre-event GW levels for individual wells and keep the aggregate ones
                              starts_with(c("gw_1d_well", "gw_4d_well")),
                              # Remove all pre-event soil var means and keep the aggregate "dry" and "wet" ones
                              contains("WWp"),
                              # Remove variables that don't likely matter to turbidity yields
                              starts_with("DO"), starts_with("Redox"),
                              # Let's use 15cm values for SoilTemp for now; remove 30cm & 45cm vars 
                              SoilTemp_pre_dry_30cm, SoilTemp_pre_wet_30cm,
                              SoilTemp_pre_dry_45cm, SoilTemp_pre_wet_45cm,
                              # For now, let's drop the aggreate soil vars for the dry site and keep the wet ones
                              # contains("pre_dry"),
                              # Drop the _1d version of variables
                              airT_1d,
                              dewPoint_1d,
                              q_1d,
                              solarRad_1d,
                              # Drop these vars that highly correlated with others
                              gw_4d_allWells,
                              SoilTemp_pre_dry_15cm,
                              gw_1d_allWells,
                              VWC_pre_dry_15cm,
                              VWC_pre_dry_30cm,
                              VWC_pre_wet_15cm,
                              VWC_pre_dry_45cm,
                              VWC_pre_wet_45cm,
                              airT_4d,
                              dewPoint_4d,
                              q_event_delta,
                              q_mm,
                              rain_int_mmPERmin_mean,
                              # I removed these after initial SOM run b/c they did not differentiate clusters
                              PET_mmHR,
                              rain_event_hrs,
                              turb_1d,
                              # Drop this complicated variable
                              q_event_dQRate_cmsPerHr,
                              # Testing the importance of DOY as a variable
                              DOY,
                              # Remove these vars b/c irrelevant for turbidity
                              SRP_1d,
                              NO3_1d
                              ))

  # Create df w/ dropped variables
  wade_drop <- wade %>% 
    # Drop variables
    select(-one_of(drop.info, drop.vars))

  # Look at correlations again without dropped variables
  wade_drop %>% 
    # Keep only complete observations/rows (no NAs in any of the columns)
    na.omit() %>% 
    lares::corr_cross(max_pvalue = 0.05, top = 30)
```
<br><br>

##### Decisions for eliminating remaining tricky variables by looking at PCA
* Variables are not transformed to improve normality
* Using Pearson correlation vs. covariance matrix given the untransformed data where vars have different units and ranges
```{r PCA figure function, include = FALSE}
# Helpful webistes
  # https://aaronschlegel.me/principal-component-analysis-r-example.html
  # http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/
  # https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance
  # https://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont

pcaCharts <- function(x) {
    vars <- apply(x$x, 2, var)
    props <- vars / sum(vars)
    df <- props %>% 
      data.frame(as.list(.)) %>% 
      select(prop_var = ".") %>% 
      as_tibble(rownames = "PC") %>% 
      mutate(PC_num = parse_number(PC)) %>% 
      mutate(prop_var_cum = cumsum(prop_var))
    
    p1 <- ggplot(data = df, aes(x = PC_num, y = prop_var)) +
      geom_point() +
      ylab("Prop. of var.") +
      theme_classic() + 
      theme(axis.title.x = element_blank())
    p2 <- ggplot(data = df, aes(x = PC_num, y = prop_var_cum)) +
      geom_point() +
      xlab("PC") +
      ylab("Cumm. prop. of var.") +
      theme_classic()
    p3 <- plot_grid(p1, p2, nrow = 2, align = "hv")
    p4 <- factoextra::fviz_pca_var(x, repel = TRUE)
    p5 <- plot_grid(p3, p4, nrow = 1, align = "v", rel_widths = c(1.5, 2))
    theme1 <- theme_classic() +
      theme(plot.title = element_text(size = 9))
    p6 <- factoextra::fviz_contrib(x, choice = "var", ggtheme = theme1, axes = 1, top = 10)
    p7 <- factoextra::fviz_contrib(x, choice = "var", ggtheme = theme1, axes = 2, top = 10)
    p8 <- factoextra::fviz_contrib(x, choice = "var", ggtheme = theme1, axes = 3, top = 10)
    p9 <- factoextra::fviz_contrib(x, choice = "var", ggtheme = theme1, axes = 4, top = 10)
    p10 <- plot_grid(p6, p7, p8, p9, nrow = 1)
    plot_grid(p5, p10, nrow = 2, rel_heights = c(2, 1))
}
```

```{r, include = TRUE, warning = FALSE, fig.width = 10, fig.height = 8}
myData <- wade_drop %>% 
  # Keep only complete observations/rows (no NAs in any of the columns)
  na.omit()

# scale = TRUE = correlation matrix; scale = FALSE = covariance matrix
  PCA <- prcomp(myData, scale = TRUE, center = TRUE)
  pcaCharts(PCA)
```
<br><br>

#### Self-organizing map (SOM)
##### Prepare data & set up grid/lattice dimensions  
We're only using complete observations/rows (no NAs in any columns)  
According to the heuristic rule from Vesanto 2000, number of grid elements/grid size/nodes = 5 * sqrt(n)  
To determine the the shape of the grid (ratio of columns to rows), we use the ratio of the first two eigen values of the input data set as recommended by Park et al. 2006  
```{r, include = TRUE, echo = FALSE}
myData <- wade_drop %>% 
  # Keep only complete observations/rows (no NAs in any of the columns)
  na.omit()

noquote(paste0("No. of complete observations: ", nobs(myData), " out of ", nobs(wade), " observations"))

# Run this code once to determine which dimensions to input into the params_SOM . . .csv file that is saved as the 'params' object below
  # According to the heuristic rule from Vesanto 2000, number of grid elements/grid size/nodes = 5 * sqrt(n)
  VesantoNodes = round(5 * sqrt(nrow(myData)), 0)
  noquote(paste0("No. of Vesanto nodes: ", VesantoNodes))
  
  # To determine the the shape of the grid, we'll determine the ratio of columns to rows using the ratio of 
  # the first two eigen values of the input data set as recommended by Park et al. 2006
  # use PCA done above
  eigenValues <- PCA$sdev^2
  first <- nth(eigenValues, 1)
  second <- nth(eigenValues, 2)
  gridRatio <- round((first/second), 1)
  noquote(paste0("Ratio of columns to rows: ", gridRatio))

  # Input this information into the params_SOM . . .csv file
```
<br><br>

##### Run SOM for a suite of grid/lattice configurations, # of nodes, and # of clusters
Code courtesy of Kristen Underwood (hidden)
```{r, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE}
# SOM Code from Kristen Underwood's sedRegData_V2.R script
# Also adapted by referencing Brittney L's Run_SOM.R script
# ------------- Define Color Palettes of SOM plots  -------------------------------------------

# Colour palette definition - this added from blog comments (found in 2014-01 CSO_SOM.R script)
# Used later in clustering plot
pretty_palette <- c("#1f77b4", '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2')

# Palette defined by kohonen package, Shane Lynn 14-01-2014, code in zip file downloaded from link in above blog
# Used later in Component Plane (HeatMap) plots
coolBlueHotRed <- function(n, alpha = 1) {
      rainbow(n, end=4/6, alpha=alpha)[n:1]
}

#barPalette=c("gray", "red", "green3", "blue","cyan", "magenta", "yellow", "black")
barPalette=c("red", "orange1", "green3", "blue","cyan", "magenta", "yellow", "black")
sedRegPallette=c("cyan", "yellow", "orange", "red", "green3", "orchid","gray", "blue")    
    
# ------------- Set Grid parameters for various Runs  -------------------------------------------

# Read in table of parameters (grid rows, cols, grid type, learning rate etc)
### IMPORTANT: cannot have number nodes greater than num observations
params <- read_csv(here("Data", "somParams", "params_SOM_wade_turb.csv"))
params <- as.matrix(params[1:nrow(params),])

# Vector of observation number from original dataset
observ <- myData %>% 
  mutate(obs = row_number()) %>% 
  dplyr::pull(obs)

# initialize matrix to store results of each SOM run
temp1 <- matrix(0, nrow(params), 5) #initialize empty matrix to append to params
Results <- cbind(params, temp1) # concatenate empty columns to copy of params matrix

# initialize matrix to store pattern to node assignments by run
clustAssignments <- matrix(0, 1, nrow(myData))

# initialize matrix to store pattern to node assignments by run
#clustMeans_Res <- matrix(0, 1, ncol(myData_num))

#loop through choices of params by Run number
# i <- 1  #use this to break for loop  #end for loop near line 410
for (i in 1:nrow(params)) {
      #### Define SOM input parameters
      myRun <- params[i,1]
      myDataSet <- params[i,2]
      myTopo <- params[i,4]
      rows <- as.numeric(params[i,5])
      cols <- as.numeric(params[i,6])
      normMeth <- params[i,7]
      wghtMeth <- params[i,8]
      niter <- as.numeric(params[i,9])  #number of iterations, 100 is default
      crsAlpha <- as.numeric(params[i,10]) #learning rate: coarse tuning phase, 0.05 is default
      finAlpha <- as.numeric(params[i,11]) #learning rate: fine tuning phase, 0.01 is default
      nclusters <- as.numeric(params[i,12])
      #for now, rest of som parameters are default values
      
     
      # -----------------Normalize the data ---------------------------------
      ### output from either is a matrix, which is format required by som()
      
      if (normMeth == "scale") {
            # Method 1 - z transformation (subtract mean, divide by stdev)
            myData.sc <- scale(myData)  #scale is a base R function
            
      } else {
            # Method 2 - L2 normalization by variable (scale between 0 and 1)
            # normMeth <- c("L2")
            # I load this up top (note from DK)
            # source("L2norm.R")  #function has been created by KU
            myData.sc <- L2norm(myData)
      }
      
      # ----------------- Weight the inputs (Optional) ---------------------------------
      
      if (wghtMeth == "PCA") {
        ### Weight the data based on the PC loadings.
        # copy the data into a new object for weighting
        myData_wghtd <- myData.sc 
        
        # Multiply the individual variables by the weights:
        
        # this "for" loop goes through each column in the data 
        # which is going to go into the SOM, matches the column name to the
        # names of the PCA loadings for PC1, and multiplies those loadings 
        # by the scaled values in the column to get weighted values:
        
        #ADD a zero weight for variable L ??
        for (w in 1:ncol(myData_wghtd)) {
          myData_wghtd[,w]<-myData_wghtd[,w] * weightsPCA[which(names(weightsPCA) == colnames(myData_wghtd)[w])]
        }
        myData.sc <- myData_wghtd
      }
      
      # ---------------------- Run the SOM ----------------------------------
      
      #set.seed(12)  #to replicate results 
      myGrid <- somgrid(cols, rows, topo = myTopo)  #function somgrid is sourced from the "class" package
      som_model <- som(X = myData.sc, grid = myGrid, rlen = niter,
                       alpha=c(0.05,0.01), 
                       keep.data = TRUE)  #run the som
      
      #, mode = c("online")  #add this as arg to som()
      #, neighbourhood.fct = c("bubble")  #add this as arg to somgrid? no need, it is default
      
      # Calculate errors
      QE <- mean(som_model$distances)  # Quantization Error
      # kohonen_2017 now generates this automatically in summary() and print()
      
      # OLD kohonen_2015 way of calculating topographic error
      #TEdist <- topo.error(som_model, type = "nodedist") #another option
      #TEbmu <- topo.error(som_model, type = "bmu") # Topographic Error
      
      # NEW kohonen_2017 way  #### 4/10: this section still needs work
      obj.dists <- object.distances(som_model, type = "data")
      # Or create An object of class "dist", which can be directly fed into 
      # (e.g.) a hierarchical clustering.
      code.dists <- object.distances(som_model, type = "codes")
      
      un.dists <- unit.distances(myGrid)
      
      # check1 <- 'check1'
      # print(check1)
      
      # ------------------- Assign Nodes to a Cluster -------------------------------------
      
      #according to dissimilarity between trained weight vectors
      #given nClusters specified in params;
      #can review performance of SOM under various cluster # by doing mult runs
      #OLD CODE
      #som_cluster <- cutree(hclust(dist(som_model$codes)), nclusters)
      #NEW CODE
      som_cluster <- cutree(hclust(code.dists), nclusters)

      # check2 <- 'check2'
      # print(check2)    
      # -------------- Visualize results in various plots (exported to pdf) --------------------------
      # Create a new folder to save the SOM plots in PDFs produced by the SOM code
      newFolder2 <- paste("Data/somResults/wade/yields/turb/", Sys.Date(), "/SOMplots_PDFs", sep = "")
      dir.create(here(newFolder2), recursive = TRUE)
      myFileName2 <- paste(myRun,"SOMplots",myDataSet,wghtMeth,normMeth, paste0(rows,"x", cols, myTopo), paste0(nclusters,"cl"), sep = "_")
      pdf(file=here(paste(newFolder2, myFileName2, sep = "/")), height=8, width=10)  #dev.off() is at 400
      
      par(mfrow=c(1,2), mar= c(5.5, 4, 4, 2) + 0.1)
      opar<-par()
      
      # Node counts
      plot(som_model, type = "count", main = "Node Counts", shape = "straight")
      mtext(paste("nodes =", (rows*cols)), side = 1, line = 1)
      mtext(paste("QE = ",round(QE, digits = 3)), side=1, line = 2)
      #mtext(paste("TEbmu =",round(TEbmu, digits = 3)), side=1, line = 3)
      
      # Quality Map
      plot(som_model, type = "quality", main = "Mapping Quality", shape = "straight")
      mtext(paste("Data Set =", myDataSet), side=1, line = 1)
      mtext(paste("Normalization Method =", normMeth), side=1, line = 2)
      mtext(paste("Weighted Inputs =", wghtMeth), side=1, line = 3)
      mtext(paste(myFileName2), side=3, outer=TRUE)
      # A good mapping should show small distances everywhere in the map". (Wehrens & Buydens, 2007)
      # mean distance of objects, mapped to a particular node, to the codebook vector of that node
      
      # check3 <- 'check3'
      # print(check3)
      
      # Training Map
      plot(som_model, type="changes", main = "Training Progress")
      
      # U-Matrix
      plot(som_model, type = "dist.neighbours", main = "U Matrix", shape = "straight", palette.name=grey.colors)
      
      # Codes/ Weight Vectors #default type = "codes"
      plot(som_model, main = "Weight Vectors", shape = "straight", codeRendering = "segments")
      
      # Hierarchical Clustering plot
      #plot(hclust(dist(som_model$codes)))   #OLD CODE
      plot(hclust(code.dists))
      
      # K-Means Clustering plot  ##NEW CODE: had to convert som_model$codes to data frame by adding as.data.frame(som_model$codes)
      wss <- (nrow(as.data.frame(som_model$codes))-1)*sum(apply(as.data.frame(som_model$codes),2,var)) 
      for (k in 2:8) {  #this can be no more than min(grid elements) minus 1 
            wss[k] <- sum(kmeans(as.data.frame(som_model$codes), centers=k)$withinss)
      }
      plot(wss, main= "K-Means Clustering")
      
      # plot the clustering results and add cluster boundaries: 
      plot(som_model, type="mapping", bgcol = barPalette[som_cluster], shape = "straight", main = "Clusters")
      add.cluster.boundaries(som_model, som_cluster) #this is a function within kohonen
      
      # U-Matrix with cluster boundaries
      plot(som_model, type = "dist.neighbours", main = "U Matrix", shape = "straight", palette.name=grey.colors)
      add.cluster.boundaries(som_model, som_cluster)
      
      # check4 <- 'check4'
      # print(check4)
      
      
      # Component Planes (Heat Maps) - Standardized Variable
      # Plot the component planes for each variable using a loop:
      ### NEW CODE: had to convert som_model$codes to a data frame in each call below
      par(mfrow = c(3, 3))
      par(cex = 0.6)
      par(mar = c(3, 3, 0, 0), oma = c(0, 0, 0, 0))
      #par(mgp = c(2, 0.5, 0))
      
      #par(mfrow=c(4,5), mai=c(0.2,0.4,0.2,0))  #mar = c(2,1,1,1), oma=c(0,0,0,0)
      for (c in 1:ncol(as.data.frame(som_model$codes))) {
            plot(som_model, type = "property", cex.lab = 1.4, border = "gray50", shape = "straight", property = as.data.frame(som_model$codes)[,c], main=colnames(as.data.frame(som_model$codes))[c], palette.name = coolBlueHotRed)
            add.cluster.boundaries(som_model, som_cluster)
      }
      
      # # # Component Planes (Heat Maps) - Unstandardized variable
      # ## NOTE: KU will need to add modification to this code from Shayne Lynn blog to address a 
      # # lattice that has empty nodes.
      # # intention is to map the average unscaled value (for a given variable) for 
      # # all input patterns mapped to each node
      # for (i in 1:ncol(som_model$codes)) {
      #       node_mns <- aggregate(as.numeric(myData_num[,i]), by=list(som_model$unit.classif), 
      #                                 FUN=mean, simplify=TRUE)[,2]
      #       #var_unscaled <- #need to fill in zero values for unoccupied nodes
      #       plot(som_model, type = "property", property=var_unscaled, main=colnames(myData_num)[i], palette.name=coolBlueHotRed)
      # }
      # 
      # # OR could try "reverse scaling" the codebook vectors stored in each node,
      # # by multiplying by the stdev of the original variables and
      # # adding the mean of the original variables
      # # perhaps there is a reverse function for scale() or I will need to calculate and store 
      # # stedev and mean in the early lines of the code for use here
      
      # Plot input patterns assigned to each node onto the clustered map
      par(mfrow=c(1,2), mar=c(6,3,3,1), oma=c(0,0,3,0))
      plot(som_model, type = "mapping" , shape = "straight", labels = som_model$unit.classif, main="Node ID Map (syst chk)") # maps node identification
      #plot(som_model, type = "mapping", labels = som_cluster, col=som_cluster+1, main="Mapping Plot")
      #add.cluster.boundaries(som_model, som_cluster)
      
      # check5 <- 'check5'
      # print(check5)      
      
      # Brittany didn't use the following chunck; commenting out for now
      ### Now match node numbers back to original [nrow] input patterns for plotting
      # newlabels <- cbind(som_model$unit.classif, observ, mySRClass)
      # plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], cex=0.75, main="Input Patterns \nMapped to each Node")
      # #plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], col = sedRegPallette[as.integer(mySRClass)], cex=0.75, main="Input Patterns \nMapped to each Node")
      # add.cluster.boundaries(som_model, som_cluster)
      
      ### Now match the cluster assignments back to the original [nrow] input patterns for plotting:
      # such as the line graphs with color coding for clusters that Peter Isles made
      # som_model$unit.classif # this is the node assignments for each data point
      # som_cluster # this is the cluster assignment for each node
      cluster_assign<-som_cluster[som_model$unit.classif] 
      # each data point, could be appended to the original dataset 
      cluster_assign
      
      # Now plot scaled variables by cluster as they differ from the mean
      par(mfrow=c(3,2), mar=c(7,3,3,1), oma=c(0,0,3,0))
      
      #p <- 1 #use this line to break loop for debugging
      for (p in 1:length(unique(cluster_assign))) {
            clusterData<- as.matrix(myData.sc[which(cluster_assign==p),], byrow = TRUE)
            if (nrow(clusterData) == 17) {
                  clusterData <- t(as.matrix(clusterData))
                  clusterMeans <- clusterData  # in case cluster has only one pattern
                  numPatterns <- 1
            } else {
                  clusterMeans<-apply(clusterData, 2, mean)
                  numPatterns <- nrow(clusterData)
            }
            
            dataMeans<-apply(myData.sc, 2, mean)  #compute overall mean for comparison
            barplot(clusterMeans-dataMeans, names.arg=names(clusterData), ylim = c(-.8,.8), las=3, main=paste("Cluster ", p, sep=""), col=barPalette[p])
            # barplot(clusterMeans-dataMeans, names.arg=names(clusterData), las=3, main=paste("Cluster ", p, sep=""), col=barPalette[p])
            mtext(paste("n =", numPatterns), side = 3, line = -0.5)
      }  #end for p loop  #ylim = c(-1.2, 1.2),
      
      #       # plot bar plots of variable values by cluster
      #       # bottom margin = 6.4 to leave room for x label with vertical hatch mark labels
      #       par(mfrow=c(3,2), mar=c(6.5,3,3,1), oma=c(0,0,3,0))
      #       for (pp in 1:length(unique(cluster_assign))) {
      #             clusterData<- as.matrix(myData.sc[which(cluster_assign==pp),], byrow = TRUE)
      #             if (nrow(clusterData) >1) {
      #                   numPatterns <- nrow(clusterData)
      #             } else {
      #                   clusterData <- t(as.matrix(clusterData)) # in case cluster has only one pattern
      #                   numPatterns <- 1
      #             }
      #             boxplot(clusterData ~ colnames(clusterData), xlab = "", ylab = "", 
      #             names=paste(colnames(clusterData)), las=2, cex=2, pch="*", col=barPalette[pp], 
      #             main=paste("Cluster ", pp, sep=""))
      #             mtext(paste("n =", numPatterns), side=3, line=0.1, at=0.0)
      # 
      #       }  #end for pp loop
      #       
      #repeat cluster plot for reference and using same color coding as bar charts
      par(mfrow=c(1,2), mar=c(6,3,3,1), oma=c(0,0,3,0))
      plot(som_model, type="mapping", shape = "straight", bgcol = barPalette[som_cluster], main = "Clusters")
      add.cluster.boundaries(som_model, som_cluster) 
      #mtext(paste("group mean - data mean for vars", normMeth), side=3, outer=TRUE)
      
      # Brittany did not use the next 2 chunks; commenting out for now
      #repeat plot of input patterns mapped to each node for ref next to cluster plot
      # plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], cex=0.75, main="Input Patterns \nMapped to each Node")
      # add.cluster.boundaries(som_model, som_cluster)
      
      #repeat large plot of input patterns mapped to each node for ref next to cluster plot
      # par(mfrow=c(1,1), mar=c(6,3,3,1), oma=c(0,0,3,0))
      # plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], col = sedRegPallette[as.integer(mySRClass)], cex=1.1, main="Input Patterns \nMapped to each Node \nColored by Mike Class")
      # add.cluster.boundaries(som_model, som_cluster)
      
      
      
      ###############
      #calculate nonparametric F statistic
      
      #first create dataframe of cluster assignments to call from the adonis2() function
      obsNum <- as.character(paste0("obs",observ))
      clustAssignDF <- as.data.frame(cbind(obsNum, cluster_assign))
      colnames(clustAssignDF) <- c('obsNum','Cluster') 
      #then run NP F statistic using adonis function of vegan package
      npFtest <- adonis2(obj.dists ~ Cluster, data=clustAssignDF, by = "terms", permutations=99)
      #extract F ratio and pVal for storage in results table
      npF <- npFtest$F[1]
      pVal_npF <- npFtest$`Pr(>F)`[1]
      
      # check6 <- 'check6'
      # print(check6)
      
      # pp <- 1 #use this line to break loop for debugging
      # #for (pp in 1:length(unique(cluster_assign))) {
      #   finalWghtsData<- as.matrix(myData.sc[which(cluster_assign==pp),], byrow = TRUE)
      #   if (nrow(clusterData) == 18) {
      #     clusterData <- t(as.matrix(clusterData))
      #     clusterMeans <- clusterData  # in case cluster has only one pattern
      #     numPatterns <- 1
      #   } else {
      #     clusterMeans<-apply(clusterData, 2, mean)
      #     numPatterns <- nrow(clusterData)
      #   }
      #   
      #   dataMeans<-apply(myData.sc, 2, mean)  #compute overall mean for comparison
      #   barplot(clusterMeans-dataMeans, ylim = c(-1, 2.5), names.arg=names(clusterData), las=3, main=paste("Cluster ", pp, sep=""), col=barPalette[pp])
      #   mtext(paste("n =", numPatterns), side = 3, line = -0.5)
      # #}  #end for pp loop
      
      
      dev.off()
      
      # ------------------- Track and Store results of each Run -------------------
      Results[i,14]<- mean(som_model$distances)
      Results[i,15] <- round(QE, digits = 6)
      Results[i,16] <- round(npF, digits = 6)
      Results[i,17] <- round(pVal_npF, digits = 6)
      
      clustAssignments <- rbind(clustAssignments, cluster_assign)
        
      
} # end for loop of SOM runs

# Print methods used
noquote(paste0("Topology: ", params[i,4]))
noquote(paste0("Data normalization method used: ", params[i,7]))
noquote(paste0("Weighting method used: ", params[i,8]))
noquote(paste0("No. of iterations: ", params[i,9]))
noquote(paste0("alphaCrs used: ", params[i,10]))
noquote(paste0("alphaFin used: ", params[i,9]))


# FORMAT & EXPORT RESULTS TABLES ----
colnames(Results) <- c('Run','DataSet','Nodes','Topol','rows','cols','normMeth', 
                       'wghtMeth','niter', 'alphaCrs', 'alphaFin', 
                       'Clusters', 'ColRowRat', 'Dist_mn','QE','npF','pVal_npF','blank') 

ResultsDF <- as_tibble(Results) %>% type_convert(col_types = cols(Run = col_double(), cols = col_double()))
write_csv(ResultsDF, here(paste0(newFolder,"/","Results_", myDataSet,".csv")))

clustAssignm <- clustAssignments[-1, ]
colnames(clustAssignm) <- as.character(paste0("obs",observ))
# runID <- as.matrix(params[,1])
# colnames(runID)<- c("Run")
clustAssignm <- cbind(Results, clustAssignm)

clustAssignmDF <- as_tibble(clustAssignm)
write_csv(clustAssignmDF, here(paste0(newFolder,"/","ClustAssign_",myDataSet,".csv")))
```
<br><br>

##### Choose the best SOM run based on non-parametric F-stat and quantization error
We want to maximize npF (ratio of b/w cluster variance) and minimize QE (mean distance b/w each data vector & best-matching unit)  
Here are the top 50% of runs based on npF
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.width = 8, out.width = "80%"}
# CHOOSE BEST LATTICE CONFIGURATION ----
  # Examine the non-parametric F-stat (npF) and quantization error (QE) to choose the best lattice configuration
  # You want to maximize npF (maximize b/w cluster variance while minimizing w/i cluster variance) &
  # Minimize QE (avg distance b/w each data vector & best-matching unit [BMU])

  # Set coefficient for second axis on plot below
  # Note: this might not be perfect and may need to adjust
  coeff <- median(ResultsDF$npF)/max(ResultsDF$QE)
  
  ResultsDF %>% 
    # Sort by npF (max to min)
    arrange(desc(npF)) %>% 
    # Only show top 50% of choices
    slice(1:(nrow(.)/2)) %>% 
    # Create lattice config ID (rows x cols)
    mutate(latID = paste(paste(rows, "x", cols, sep = ""), "_", Clusters, "cl", "_", Run, sep = "")) %>% 
    # Plot
    ggplot(aes(x = reorder(latID, -npF))) +
    geom_bar(aes(y = npF, fill = "npF"), stat= "identity") +
    geom_line(aes(y = QE * coeff, group = 1, color = "QE")) +
    geom_point(aes(y = QE * coeff, group = 1, color = "QE"), size = 2) +
    scale_y_continuous(
      name = "npF",
      sec.axis = sec_axis(~. / coeff, name = "QE")
    ) +
    scale_fill_manual(values = "gray70") +
    scale_color_manual(values = "blue") +
    xlab("Lattice config. & clusters (rows x cols _ clusters_run)") +
    theme_bw() +
    theme(legend.title = element_blank(),
          legend.spacing = unit(-0.5, "lines"),
          panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90, vjust = 0.5),
          axis.title.x = element_text(margin = margin(t = 5, r = 0, b = 0, l = 0)))  
  
  # Save plot
  # ggsave(here(newFolder, "npFvsQE.pdf"), device = "pdf", height = 4, width = 6, units = "in", dpi = 75)
```
<br><br>

##### Choose the best run from each high performing (top 50%) cluster #
```{r, include = TRUE, echo = FALSE, message = FALSE}
# Automated
# best_summ_top <-
#   ResultsDF %>%
#   select(Run, rows, cols, Nodes, Clusters, npF, QE) %>%
#   # Sort by npF (max to min)
#   arrange(desc(npF)) %>%
#   # Only look at the first 25
#   slice(1:30) %>%
#   # Arrange by QE (min to max)
#   arrange(Clusters, QE) %>%
#   # Select the top (lowest QE) for each cluster #
#   group_by(Clusters) %>%
#   slice(1:1) %>%
#   # Sort by npF (max to min)
#   arrange(desc(npF)) %>%
#   ungroup()

# Manual selection
best_summ_top <- 
  ResultsDF %>% 
  select(Run, rows, cols, Nodes, Clusters, npF, QE) %>% 
  # filter(Run %in% c(17, 7, 15))
  filter(Run %in% c(4, 39, 45))
  
# noquote("These were the top runs for each high-performing (top 33%) cluster #:")
best_summ_top %>% 
  mutate_at(vars(c(npF, QE)),
            .funs = ~round(., 3)) %>% 
  arrange(Clusters) %>% 
  gt()

# Select the top 4 cluster
best_summ_top4 <-
  best_summ_top %>% 
  slice(1:1)

# Select the top 5 cluster
best_summ_top5 <-
  best_summ_top %>% 
  slice(2:2)

# Select the best 6 cluster
best_summ_top6 <-
  best_summ_top %>% 
  slice(3:3)

# Which run is best?
  best_run_4cl = best_summ_top4$Run
  best_run_5cl = best_summ_top5$Run
  best_run_6cl = best_summ_top6$Run

# CREATE DF WITH EVENT IDs & CLUSTER #'s ----  
datWithCluster <- clustAssignmDF %>% 
  # Parse columns into correct type
  type_convert() %>% 
  filter(Run == best_run_4cl | Run == best_run_5cl | Run == best_run_6cl) %>% 
  # Pivot to longer format
  pivot_longer(cols = obs1:ncol(.), names_to = "obs", values_to = "cluster") %>% 
  select(Clusters, obs, cluster) %>% 
  pivot_wider(names_from = Clusters, values_from = cluster) %>% 
  rename(clust_wat_4cl = `4`, clust_wat_5cl = `5`, clust_wat_6cl = `6`) %>% 
  # Bind cluster ID to myData
  bind_cols(myData) %>% 
  # Join to original data to get event IDs
  left_join(wade) %>% 
  mutate(event_start = ymd_hms(event_start, tz = "Etc/GMT+4")) %>% 
  # Drop all the independent variables you didn't use in the SOM (copy from myData above); 
  # but KEEP the INFO vars
  select(-one_of(drop.vars)) %>% 
  # Arrange columns
  select(site:ncol(.), everything()) %>% 
  select(site:season, clust_wat_4cl, clust_wat_5cl, clust_wat_6cl, everything()) %>% 
  select(-obs) %>% 
  # Add groundwater data
  left_join(wade %>% select(site, event_start, starts_with(c("gw_1d", "gw_4d"))))
  
# Write this results DF to CSV
datWithCluster %>% 
  mutate(event_start = as.character(event_start)) %>% 
  write_csv(here(paste0(newFolder,"/","Results_", myDataSet, "_", "withClusters",".csv")))
```
<br><br>

```{r plot themes and labels, include = FALSE}
# Plotting specifics
  theme1 <- theme_classic() +
            theme(axis.text = element_text(size = 11),
                  axis.title = element_text(size = 12),
                  axis.title.x = element_text(margin=margin(5,0,0,0)),
                  axis.title.y = element_text(margin=margin(0,5,0,0)),
                  legend.title = element_text(size = 9),
                  legend.text = element_text(size = 9))
```

To examine this run in greater detail (e.g., component planes), see the 'X_SOMplots_site_ ... .pdf'  
<br><br>

##### 4 cluster model - examine boxplots of independent variables by cluster
```{r , include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.4}
datWithCluster %>% 
  select(-one_of(drop.vars)) %>%
  # pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
  pivot_longer(cols = rain_event_total_mm:ncol(.), names_to = "var", values_to = "value") %>% 
  ggplot(aes(x = clust_wat_4cl, y = value, group = clust_wat_4cl)) +
  facet_wrap(~var, scales = "free_y", ncol = 4) +
  geom_boxplot(fill = "white") +
  geom_point(position=position_jitter(width=0.2), shape=1, size=1, color="gray50", alpha=0.6) +
  ylab("Value") + xlab("Cluster") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        strip.text = element_text(size = 7),
        panel.spacing.x = unit(0.3, "inches"),
        panel.spacing.y = unit(0.45, "inches"))
```
<br><br>

##### 4 cluster model - examine z-scores by cluster
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.2}
# Again, only look at vars used in SOM
datWithCluster %>% 
  select(-one_of(drop.vars)) %>% 
  # Rename clusters
  mutate(cluster = paste("Cluster", clust_wat_4cl, sep = " ")) %>% 
  # Calculate z-scores for each independent variable
  select(cluster, everything()) %>% 
  # mutate_at(vars(c(DOY:ncol(.))),
  #         .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  # pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
  mutate_at(vars(c(rain_event_total_mm:ncol(.))),
          .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  pivot_longer(cols = rain_event_total_mm:ncol(.), names_to = "var", values_to = "value") %>%   
  group_by(cluster, var) %>% 
  summarize(median = median(value, na.rm = T),
            q2 = quantile(value, 1/4, na.rm = T),
            q3 = quantile(value, 3/4, na.rm = T)) %>%   
  # Order the vars n var
  mutate(var = factor(var, levels = c("rain_int_mmPERmin_mean", "rain_int_mmPERmin_max", "rain_event_hrs", "rain_event_total_mm",
                                      "q_4d", "q_event_max", "time_sinceLastEvent", "API_4d", "VWC_pre_wet_15cm", "VWC_pre_wet_30cm", "VWC_pre_dry_15cm", "VWC_pre_dry_30cm", "SoilTemp_pre_wet_15cm",
                                      "DO_pre_dry_15cm", "DO_pre_wet_15cm", "Redox_pre_dry_15cm", "Redox_pre_wet_15cm", 
                                      "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY", "NO3_1d", "SRP_1d", "turb_1d"))) %>%
  
  
  ggplot(aes(x = var, y = median)) +
    # Must use scales = "free" when using the reordering functions
    facet_wrap(~cluster, ncol = 2) +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymin = q2, ymax = q3), width = 0.2, position = position_dodge(0.9)) +
    # scale_x_reordered() +
    ylab("Median z-scores") +
    coord_flip() +
    theme_bw() +
    theme(panel.grid = element_blank(),
          axis.title.y = element_blank()) +
  ggtitle("Characteristics of each cluster - median z-scores")
```
<br><br>

##### 5 cluster model - examine boxplots of independent variables by cluster
```{r , include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.4}
datWithCluster %>% 
  select(-one_of(drop.vars)) %>% 
  # pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
  pivot_longer(cols = rain_event_total_mm:ncol(.), names_to = "var", values_to = "value") %>% 
  ggplot(aes(x = clust_wat_5cl, y = value, group = clust_wat_5cl)) +
  facet_wrap(~var, scales = "free_y", ncol = 4) +
  geom_boxplot(fill = "white") +
  geom_point(position=position_jitter(width=0.2), shape=1, size=1, color="gray50", alpha=0.6) +
  ylab("Value") + xlab("Cluster") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        strip.text = element_text(size = 7),
        panel.spacing.x = unit(0.3, "inches"),
        panel.spacing.y = unit(0.45, "inches"))
```
<br><br>

##### 5-cluster model - examine z-scores by cluster
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.2}
# Again, only look at vars used in SOM
datWithCluster %>% 
  select(-one_of(drop.vars)) %>% 
  # Rename clusters
  mutate(cluster = paste("Cluster", clust_wat_5cl, sep = " ")) %>% 
  # Calculate z-scores for each independent variable
  select(cluster, everything()) %>% 
  # mutate_at(vars(c(DOY:ncol(.))),
  #         .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  # pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
  mutate_at(vars(c(rain_event_total_mm:ncol(.))),
          .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  pivot_longer(cols = rain_event_total_mm:ncol(.), names_to = "var", values_to = "value") %>%   
  group_by(cluster, var) %>% 
  summarize(median = median(value, na.rm = T),
            q2 = quantile(value, 1/4, na.rm = T),
            q3 = quantile(value, 3/4, na.rm = T)) %>%   
  # Order the vars n var
  mutate(var = factor(var, levels = c("rain_int_mmPERmin_mean", "rain_int_mmPERmin_max", "rain_event_hrs", "rain_event_total_mm",
                                      "q_4d", "q_event_max", "time_sinceLastEvent", "API_4d", "VWC_pre_wet_15cm", "VWC_pre_wet_30cm", "VWC_pre_dry_15cm", "VWC_pre_dry_30cm", "SoilTemp_pre_wet_15cm",
                                      "DO_pre_dry_15cm", "DO_pre_wet_15cm", "Redox_pre_dry_15cm", "Redox_pre_wet_15cm", 
                                      "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY", "NO3_1d", "SRP_1d", "turb_1d"))) %>%
  ggplot(aes(x = var, y = median)) +
    # Must use scales = "free" when using the reordering functions
    facet_wrap(~cluster, ncol = 2) +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymin = q2, ymax = q3), width = 0.2, position = position_dodge(0.9)) +
    # scale_x_reordered() +
    ylab("Median z-scores") +
    coord_flip() +
    theme_bw() +
    theme(panel.grid = element_blank(),
          axis.title.y = element_blank()) +
  ggtitle("Characteristics of each cluster - median z-scores")
```
<br><br>

##### 6 cluster model - examine boxplots of independent variables by cluster
```{r , include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.4}
datWithCluster %>% 
  select(-one_of(drop.vars)) %>% 
  # pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
  pivot_longer(cols = rain_event_total_mm:ncol(.), names_to = "var", values_to = "value") %>% 
  ggplot(aes(x = clust_wat_6cl, y = value, group = clust_wat_6cl)) +
  facet_wrap(~var, scales = "free_y", ncol = 4) +
  geom_boxplot(fill = "white") +
  geom_point(position=position_jitter(width=0.2), shape=1, size=1, color="gray50", alpha=0.6) +
  ylab("Value") + xlab("Cluster") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        strip.text = element_text(size = 7),
        panel.spacing.x = unit(0.3, "inches"),
        panel.spacing.y = unit(0.45, "inches"))
```
<br><br>

##### 6-cluster model - examine z-scores by cluster
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.2}
# Again, only look at vars used in SOM
datWithCluster %>% 
  select(-one_of(drop.vars)) %>% 
  # Rename clusters
  mutate(cluster = paste("Cluster", clust_wat_6cl, sep = " ")) %>% 
  # Calculate z-scores for each independent variable
  select(cluster, everything()) %>% 
  # mutate_at(vars(c(DOY:ncol(.))),
  #         .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  # pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
  mutate_at(vars(c(rain_event_total_mm:ncol(.))),
          .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  pivot_longer(cols = rain_event_total_mm:ncol(.), names_to = "var", values_to = "value") %>%   
  group_by(cluster, var) %>% 
  summarize(median = median(value, na.rm = T),
            q2 = quantile(value, 1/4, na.rm = T),
            q3 = quantile(value, 3/4, na.rm = T)) %>%   
  # Order the vars n var
  mutate(var = factor(var, levels = c("rain_int_mmPERmin_mean", "rain_int_mmPERmin_max", "rain_event_hrs", "rain_event_total_mm",
                                      "q_4d", "q_event_max", "time_sinceLastEvent", "API_4d", "VWC_pre_wet_15cm", "VWC_pre_wet_30cm", "VWC_pre_dry_15cm", "VWC_pre_dry_30cm", "SoilTemp_pre_wet_15cm",
                                      "DO_pre_dry_15cm", "DO_pre_wet_15cm", "Redox_pre_dry_15cm", "Redox_pre_wet_15cm", 
                                      "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY", "NO3_1d", "SRP_1d", "turb_1d"))) %>%
  ggplot(aes(x = var, y = median)) +
    # Must use scales = "free" when using the reordering functions
    facet_wrap(~cluster, ncol = 2) +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymin = q2, ymax = q3), width = 0.2, position = position_dodge(0.9)) +
    # scale_x_reordered() +
    ylab("Median z-scores") +
    coord_flip() +
    theme_bw() +
    theme(panel.grid = element_blank(),
          axis.title.y = element_blank()) +
  ggtitle("Characteristics of each cluster - median z-scores")
```
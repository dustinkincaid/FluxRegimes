---
title: <font size = "5"> Self-organizing map analysis for Hungerford </font>
author: "Dustin Kincaid"
date: "12/10/2020<br><br>"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r notes, include=FALSE}
# A helpful website about Rmarkdown
  # https://ourcodingclub.github.io/tutorials/rmarkdown/

# Can't have a lattice configuration with more nodes than observations in the analysis
```

##### If you're reading this for the 1st time, I recommend you read the Objectives and look at the resulting figures at the end of the document to get an idea of what I'm doing before digging too deeply into the methods; that said, DO dig into the methods afterwards
<br><br>

#### Objective  
Cluster high-flow event characteristics and antecedent watershed conditions to evaluate how these factors converge as flux regimes (clusters) to produce variability in:  
1. Event NO3 yields  
2. Event SRP yields  
3. Event turbidity yields  
4. Event NO3:SRP yield ratios   
<br><br>

```{r setup, include=FALSE, warning=FALSE, message = FALSE}
# Set defaults
knitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, out.width = "70%", fig.align = "center") 

# Load packages ----
  library("here")      # to make file paths fail less
  library("tidyverse") # general workhorse
  library("lubridate") # to work with dates
  library("kohonen")   # to run the SOM
  library("vegan")     # to run adonis() for calc of nonparametric F test
  # devtools::install_github("laresbernardo/lares")
  library("lares")     # examine var correlations
  library("ggforce")   # for facet plots on multiple pages
  library("gt")       # print pretty tables
  library("cowplot")  # plotting assistance
  library("grid")     
  library("gridExtra")  

  # Load the range normalization function written by Kristin Underwood
  source(here("Scripts", "L2norm.R"))

# Create a new folder to save the plots and CSVs produced by the SOM code
newFolder <- paste("Data/somResults/hford/", Sys.Date(), sep = "")
dir.create(here(newFolder), recursive = TRUE)

# Give a site label to file names produced in SOM code
  myDataSet <- c("hford")

# Read in data
  # Calculated event metrics for each site as calculated in compile_calculate_allVars.R
  hford <- read_csv(here("Data", "eventMetrics_hford.csv")) %>% 
    mutate(event_start = ymd_hms(event_start, tz = "Etc/GMT+4")) %>% 
    # Code the multipeak col as numerical
    mutate(multipeak = ifelse(multipeak == "NO", 0, 1))
  
  # Calculated hysteresis indices from calculate_hysteresis.R
  hyst <- read_csv(here("Data", "hysteresis_indices.csv")) %>%
    mutate(event_start = ymd_hms(event_start, tz = "Etc/GMT+4"),
           event_end = ymd_hms(event_end, tz = "Etc/GMT+4"))   
```

#### Select variables to keep
> Per K Underwood: if two variables are strongly correlated (negatively or positively) they can effectively "double-weight" a particular factor important in driving
clustering; thus, keep just one of the variables to serve as a proxy for that factor

```{r, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, out.width = "80%"}
# Select vars to keep Step 1
# List the variables that will not be used in the analysis (e.g., response/dependent & ID/INFO)
drop.info <- names(hford %>% 
                     select(# Remove the non-numerical ID/INFO
                            site, season, event_start,
                            # Remove response variables
                            NO3_kg_km2, SRP_kg_km2, event_NO3_SRP, turb_kg_km2))

# Look at correlations between variables
hford %>% 
  # Remove the vars listed above
  select(-one_of(drop.info)) %>% 
  # Remove all rain_Xd vars and keep the antecedent precipitation index (API) instead
  select(-c(rain_1d:rain_30d)) %>% 
  # Removal all GW event data (e.g., max and change); keep pre-event level data for now
  # Keeping GW level data limits the # of events we cluster b/c no GW data in 2017
  # Below I use a VWC variable as a proxy for pre-event GW levels and remove these
  select(-c(starts_with("gw_event_"))) %>% 
  # Remove all pre-event GW levels for individual wells and keep the aggregate ones
  select(-c(starts_with(c("gw_1d_well", "gw_4d_well")))) %>% 
  # Remove all pre-event soil var means and keep the aggregate "dry" and "wet" ones
  select(-c(contains("HWp"))) %>% 
  # Let's use 15cm values for soil vars for now; remove all 30cm & 45cm aggreate soil vars except VWC
  select(-c(DO_pre_dry_30cm, Redox_pre_dry_30cm, SoilTemp_pre_dry_30cm,
            DO_pre_wet_30cm, Redox_pre_wet_30cm, SoilTemp_pre_wet_30cm,
            DO_pre_dry_45cm, Redox_pre_dry_45cm, SoilTemp_pre_dry_45cm,
            DO_pre_wet_45cm, Redox_pre_wet_45cm, SoilTemp_pre_wet_45cm)) %>%
  # For now, let's drop the aggreate soil vars for the dry site and keep the wet ones
  select(-c(contains("pre_dry"))) %>% 
  # Keep only gw well 5 for now
  # select(-c(ends_with("well1"))) %>% 
  # Keep only complete observations/rows (no NAs in any of the columns)
  na.omit() %>% 
  # Visualize these
  lares::corr_cross(max_pvalue = 0.05, top = 30)
```

```{r, include = FALSE}
# Look at plots of correlated variables
  # GW level ~ VWC
  hford %>%
    # pivot_longer(cols = c(VWC_pre_pit1_15cm, VWC_pre_pit1_30cm, VWC_pre_pit1_45cm, VWC_pre_pit3_15cm, VWC_pre_pit3_30cm, VWC_pre_pit3_45cm), names_to = "var", values_to = "level") %>%
    pivot_longer(cols = c(VWC_pre_wet_15cm, VWC_pre_wet_30cm), names_to = "var", values_to = "VWC") %>%
    ggplot(aes(x = gw_4d_allWells, y = VWC, color = var)) +
    geom_point(shape = 1) +
    geom_abline(slope = 1) +
    theme_classic()

  # Other correlations
  # scatter plot comparison
  hford %>% 
    ggplot(aes(x = rain_int_mmPERmin_mean, y = rain_int_mmPERmin_max)) +
    geom_point(shape = 1) +
    geom_abline(slope = 1) +
    geom_smooth(method = "lm") +
    theme_classic()
  # boxplot comparison
  hford %>% 
    pivot_longer(cols = c(rain_int_mmPERmin_mean, rain_int_mmPERmin_max), names_to = "var", values_to = "value") %>% 
    ggplot(aes(x = var, y = value)) +
    geom_boxplot()
  
  hford %>% 
    ggplot(aes(x = rain_int_mmPERmin_max)) +
    geom_histogram()
```

##### Decisions for eliminating variables w/ correlations >70%
These were used for the 2020-12-10 run:  

* **These decisions were tough to make and need review**
    + rain_event_total_mm and API_4d are highly correlated (88%), but represent different things; I'm going to leave both for now, but could try versions w/ one or the other to see if it matters
```{r, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
# rain_event_total_mm ~ API_4d
hford %>% 
  ggplot(aes(x = rain_event_total_mm, y = API_4d)) +
  geom_point(shape = 1) +
  geom_abline(slope = 1) +
  geom_smooth(method = "lm") +
  theme_classic()
```
<br><br>    

* **Tough decisions that need review (continued)**
    + q_event_max and q_mm are highly correlated (83.8%); let's drop q_event_max for now, because?
    + SoilTemp_pre_wet_15cm and VWC_pre_wet_15cm are correlated (72.3%), but they're pretty different and close enough to 70% correlation cutoff; will leave both for now and could try versions w/ one or the other to see if it matters
<br><br>

* **I feel OK about these decisions, but they should be reviewed as well**
* If the 1-d and 4-d values for a variable are highly correlated, use the 4-d value  
    + gw_1d_allWells and gw_4d_allWells are highly correlated (99.2%); remove gw_1d_allWells  
* Trying to find a VWC variable that correlates well with GW level so that I can remove GW level vars (no GW data in 2017)
    + when dropping all GW variables; n obs increases from 45 to 51  
    + gw_4d_allWells and VWC_pre_wet_30cm most highly correlated (94.5%); same with VWC_pre_wet_15cm (94.2%), except slightly less linear at higher values
    + drop gw_4d_allWells and use VWC as a proxy for GW level  
    + VWC_pre_wet_15cm and VWC_pre_wet_30cm highly correlated (94.7%)  
    + I want to keep all 15cm values for soil vars including VWC, so for now we'll drop VWC_pre_wet_30cm to avoid double-weighting VWC
* MET variables
    + airT_1d and airT_4d are highly correlated (90.1%); removing airT_1d
    + airT_4d and dewPoint_4d are highly correlated (96.6%), as is dewPoint_1d; removing both dewPoints
    + airT_4d and SoilTemp_pre_wet_15cm are highly correlated (92.9%); drop airT_4d b/c we still have diff_airT_soilT
    + solarRad_1d and solarRad_4d are highly correlated (74.7%); drop solarRad_1d per rule above
* Q  
    + q_event_delta & q_event_max are highly correlated (96.3%); q_event_max is more normally distributed, so let's keep it vs delta  
    + q_1d and q_4d are highly correlated (86.4%), so sticking with rule above will keep the q_4d  
    + Drop q_event_dQRate_cmsPerHr b/c it's confusing and hopefully q_event_delta or rain intensity will capture this      
* Redox  
    + If redox variables prove not to be important or they are highly correlated with another variable, we can remove them and increase n obs by at least 7  
* Rain  
    + Drop all the rain_Xd vars, b/c API_4d should cover this, though would be interesting to test how many days pre-event (e.g., 4 days for API) matters  
    + rain_int_mmPERmin_mean and rain_int_mmPERmin_max are correlated (74.4%); drop _max and so we can keep the mean intensity of the rain event
* Stream
    + turb_1d proved to be unuseful in driving clusters in SOM, so I removed it
<br><br>

##### Look at correlations again after dropping variables
```{r, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, out.width = "80%"}
# Look at correlations again without dropped variables
  # List the additional dropped variables
  drop.vars <- names(hford %>% 
                       select(# Remove all rain_Xd vars and keep API instead
                              rain_1d:rain_30d,
                              # Removal all GW event data (e.g., max and change)
                              starts_with("gw_event_"),
                              # Remove all pre-event GW levels for individual wells and keep the aggregate ones
                              starts_with(c("gw_1d_well", "gw_4d_well")),
                              # Remove all pre-event soil var means and keep the aggregate "dry" and "wet" ones
                              contains("HWp"),
                              # Remove all 30cm & 45cm aggreate soil vars except VWC
                              DO_pre_dry_30cm, Redox_pre_dry_30cm, SoilTemp_pre_dry_30cm,
                              DO_pre_wet_30cm, Redox_pre_wet_30cm, SoilTemp_pre_wet_30cm,
                              DO_pre_dry_45cm, Redox_pre_dry_45cm, SoilTemp_pre_dry_45cm,
                              DO_pre_wet_45cm, Redox_pre_wet_45cm, SoilTemp_pre_wet_45cm,
                              # For now, let's drop the aggreate soil vars for the dry site and keep the wet ones
                              contains("pre_dry"),
                              # Drop the _1d version of variables
                              gw_1d_allWells,
                              q_1d,
                              airT_1d,
                              dewPoint_1d,
                              solarRad_1d,
                              # Drop these vars that highly correlated with others
                              gw_1d_allWells,
                              gw_4d_allWells,
                              VWC_pre_wet_30cm,
                              VWC_pre_wet_45cm,
                              airT_4d,
                              dewPoint_4d,
                              q_event_delta,
                              q_event_max,
                              rain_int_mmPERmin_max,
                              # Drop this complicated variable
                              q_event_dQRate_cmsPerHr,
                              # Drop these variables, which proved unuseful in the SOM,
                              turb_1d))

  # Create df w/ dropped variables
  hford_drop <- hford %>% 
    # Drop variables
    select(-one_of(drop.info, drop.vars))

  # Look at correlations again without dropped variables
  hford_drop %>% 
    # Keep only complete observations/rows (no NAs in any of the columns)
    na.omit() %>% 
    lares::corr_cross(max_pvalue = 0.05, top = 30)
```

#### Self-organizing map (SOM)
##### Prepare data & set up grid/lattice dimensions  
We're only using complete observations/rows (no NAs in any columns)  
According to the heuristic rule from Vesanto 2000, number of grid elements/grid size/nodes = 5 * sqrt(n)  
To determine the the shape of the grid (ratio of columns to rows), we use the ratio of the first two eigen values of the input data set as recommended by Park et al. 2006  
```{r, include = TRUE, echo = FALSE}
myData <- hford_drop %>% 
  # Keep only complete observations/rows (no NAs in any of the columns)
  na.omit()

noquote(paste0("No. of complete observations: ", nobs(myData), " out of ", nobs(hford), " observations"))

# Run this code once to determine which dimensions to input into the params_SOM . . .csv file that is saved as the 'params' object below
  # According to the heuristic rule from Vesanto 2000, number of grid elements/grid size/nodes = 5 * sqrt(n)
  VesantoNodes = round(5 * sqrt(nrow(myData)), 0)
  noquote(paste0("No. of Vesanto nodes: ", VesantoNodes))
  
  # To determine the the shape of the grid, we'll determine the ratio of columns to rows using the ratio of 
  # the first two eigen values of the input data set as recommended by Park et al. 2006
  # PCA
  # scale = TRUE = correlation matrix; scale = FALSE = covariance matrix
  PCA <- prcomp(myData, scale = TRUE, center = TRUE)
  eigenValues <- PCA$sdev^2
  first <- nth(eigenValues, 1)
  second <- nth(eigenValues, 2)
  gridRatio <- round((first/second), 1)
  noquote(paste0("Ratio of columns to rows: ", gridRatio))

  # Input this information into the params_SOM . . .csv file
```
<br><br>

##### Run SOM for a suite of grid/lattice configurations, # of nodes, and # of clusters
Code courtesy of Kristen Underwood (hidden)
```{r, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE}
# SOM Code from Kristen Underwood's sedRegData_V2.R script
# Also adapted by referencing Brittney L's Run_SOM.R script
# ------------- Define Color Palettes of SOM plots  -------------------------------------------

# Colour palette definition - this added from blog comments (found in 2014-01 CSO_SOM.R script)
# Used later in clustering plot
pretty_palette <- c("#1f77b4", '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2')

# Palette defined by kohonen package, Shane Lynn 14-01-2014, code in zip file downloaded from link in above blog
# Used later in Component Plane (HeatMap) plots
coolBlueHotRed <- function(n, alpha = 1) {
      rainbow(n, end=4/6, alpha=alpha)[n:1]
}

#barPalette=c("gray", "red", "green3", "blue","cyan", "magenta", "yellow", "black")
barPalette=c("red", "orange1", "green3", "blue","cyan", "magenta", "yellow", "black")
sedRegPallette=c("cyan", "yellow", "orange", "red", "green3", "orchid","gray", "blue")    
    
# ------------- Set Grid parameters for various Runs  -------------------------------------------

# Read in table of parameters (grid rows, cols, grid type, learning rate etc)
### IMPORTANT: cannot have number nodes greater than num observations
params <- read_csv(here("Data", "somParams", "params_SOM_hford_2020-12-10.csv"))
params <- as.matrix(params[1:nrow(params),])

# Vector of observation number from original dataset
observ <- myData %>% 
  mutate(obs = row_number()) %>% 
  dplyr::pull(obs)

# initialize matrix to store results of each SOM run
temp1 <- matrix(0, nrow(params), 5) #initialize empty matrix to append to params
Results <- cbind(params, temp1) # concatenate empty columns to copy of params matrix

# initialize matrix to store pattern to node assignments by run
clustAssignments <- matrix(0, 1, nrow(myData))

# initialize matrix to store pattern to node assignments by run
#clustMeans_Res <- matrix(0, 1, ncol(myData_num))

#loop through choices of params by Run number
# i <- 1  #use this to break for loop  #end for loop near line 410
for (i in 1:nrow(params)) {
      #### Define SOM input parameters
      myRun <- params[i,1]
      myDataSet <- params[i,2]
      myTopo <- params[i,4]
      rows <- as.numeric(params[i,5])
      cols <- as.numeric(params[i,6])
      normMeth <- params[i,7]
      wghtMeth <- params[i,8]
      niter <- as.numeric(params[i,9])  #number of iterations, 100 is default
      crsAlpha <- as.numeric(params[i,10]) #learning rate: coarse tuning phase, 0.05 is default
      finAlpha <- as.numeric(params[i,11]) #learning rate: fine tuning phase, 0.01 is default
      nclusters <- as.numeric(params[i,12])
      #for now, rest of som parameters are default values
      
     
      # -----------------Normalize the data ---------------------------------
      ### output from either is a matrix, which is format required by som()
      
      if (normMeth == "scale") {
            # Method 1 - z transformation (subtract mean, divide by stdev)
            myData.sc <- scale(myData)  #scale is a base R function
            
      } else {
            # Method 2 - L2 normalization by variable (scale between 0 and 1)
            # normMeth <- c("L2")
            # I load this up top (note from DK)
            # source("L2norm.R")  #function has been created by KU
            myData.sc <- L2norm(myData)
      }
      
      # ----------------- Weight the inputs (Optional) ---------------------------------
      
      if (wghtMeth == "PCA") {
        ### Weight the data based on the PC loadings.
        # copy the data into a new object for weighting
        myData_wghtd <- myData.sc 
        
        # Multiply the individual variables by the weights:
        
        # this "for" loop goes through each column in the data 
        # which is going to go into the SOM, matches the column name to the
        # names of the PCA loadings for PC1, and multiplies those loadings 
        # by the scaled values in the column to get weighted values:
        
        #ADD a zero weight for variable L ??
        for (w in 1:ncol(myData_wghtd)) {
          myData_wghtd[,w]<-myData_wghtd[,w] * weightsPCA[which(names(weightsPCA) == colnames(myData_wghtd)[w])]
        }
        myData.sc <- myData_wghtd
      }
      
      # ---------------------- Run the SOM ----------------------------------
      
      #set.seed(12)  #to replicate results 
      myGrid <- somgrid(cols, rows, topo = myTopo)  #function somgrid is sourced from the "class" package
      som_model <- som(X = myData.sc, grid = myGrid, rlen = niter,
                       alpha=c(0.05,0.01), 
                       keep.data = TRUE)  #run the som
      
      #, mode = c("online")  #add this as arg to som()
      #, neighbourhood.fct = c("bubble")  #add this as arg to somgrid? no need, it is default
      
      # Calculate errors
      QE <- mean(som_model$distances)  # Quantization Error
      # kohonen_2017 now generates this automatically in summary() and print()
      
      # OLD kohonen_2015 way of calculating topographic error
      #TEdist <- topo.error(som_model, type = "nodedist") #another option
      #TEbmu <- topo.error(som_model, type = "bmu") # Topographic Error
      
      # NEW kohonen_2017 way  #### 4/10: this section still needs work
      obj.dists <- object.distances(som_model, type = "data")
      # Or create An object of class "dist", which can be directly fed into 
      # (e.g.) a hierarchical clustering.
      code.dists <- object.distances(som_model, type = "codes")
      
      un.dists <- unit.distances(myGrid)
      
      # check1 <- 'check1'
      # print(check1)
      
      # ------------------- Assign Nodes to a Cluster -------------------------------------
      
      #according to dissimilarity between trained weight vectors
      #given nClusters specified in params;
      #can review performance of SOM under various cluster # by doing mult runs
      #OLD CODE
      #som_cluster <- cutree(hclust(dist(som_model$codes)), nclusters)
      #NEW CODE
      som_cluster <- cutree(hclust(code.dists), nclusters)

      # check2 <- 'check2'
      # print(check2)    
      # -------------- Visualize results in various plots (exported to pdf) --------------------------
      # Create a new folder to save the SOM plots in PDFs produced by the SOM code
      newFolder2 <- paste("Data/somResults/hford/", Sys.Date(), "/SOMplots_PDFs", sep = "")
      dir.create(here(newFolder2), recursive = TRUE)
      myFileName2 <- paste(myRun,"SOMplots",myDataSet,wghtMeth,normMeth, paste0(rows,"x", cols, myTopo), paste0(nclusters,"cl"), sep = "_")
      pdf(file=here(paste(newFolder2, myFileName2, sep = "/")), height=8, width=10)  #dev.off() is at 400
      
      par(mfrow=c(1,2), mar= c(5.5, 4, 4, 2) + 0.1)
      opar<-par()
      
      # Node counts
      plot(som_model, type = "count", main = "Node Counts", shape = "straight")
      mtext(paste("nodes =", (rows*cols)), side = 1, line = 1)
      mtext(paste("QE = ",round(QE, digits = 3)), side=1, line = 2)
      #mtext(paste("TEbmu =",round(TEbmu, digits = 3)), side=1, line = 3)
      
      # Quality Map
      plot(som_model, type = "quality", main = "Mapping Quality", shape = "straight")
      mtext(paste("Data Set =", myDataSet), side=1, line = 1)
      mtext(paste("Normalization Method =", normMeth), side=1, line = 2)
      mtext(paste("Weighted Inputs =", wghtMeth), side=1, line = 3)
      mtext(paste(myFileName2), side=3, outer=TRUE)
      # A good mapping should show small distances everywhere in the map". (Wehrens & Buydens, 2007)
      # mean distance of objects, mapped to a particular node, to the codebook vector of that node
      
      # check3 <- 'check3'
      # print(check3)
      
      # Training Map
      plot(som_model, type="changes", main = "Training Progress")
      
      # U-Matrix
      plot(som_model, type = "dist.neighbours", main = "U Matrix", shape = "straight", palette.name=grey.colors)
      
      # Codes/ Weight Vectors #default type = "codes"
      plot(som_model, main = "Weight Vectors", shape = "straight", codeRendering = "segments")
      
      # Hierarchical Clustering plot
      #plot(hclust(dist(som_model$codes)))   #OLD CODE
      plot(hclust(code.dists))
      
      # K-Means Clustering plot  ##NEW CODE: had to convert som_model$codes to data frame by adding as.data.frame(som_model$codes)
      wss <- (nrow(as.data.frame(som_model$codes))-1)*sum(apply(as.data.frame(som_model$codes),2,var)) 
      for (k in 2:8) {  #this can be no more than min(grid elements) minus 1 
            wss[k] <- sum(kmeans(as.data.frame(som_model$codes), centers=k)$withinss)
      }
      plot(wss, main= "K-Means Clustering")
      
      # plot the clustering results and add cluster boundaries: 
      plot(som_model, type="mapping", bgcol = barPalette[som_cluster], shape = "straight", main = "Clusters")
      add.cluster.boundaries(som_model, som_cluster) #this is a function within kohonen
      
      # U-Matrix with cluster boundaries
      plot(som_model, type = "dist.neighbours", main = "U Matrix", shape = "straight", palette.name=grey.colors)
      add.cluster.boundaries(som_model, som_cluster)
      
      # check4 <- 'check4'
      # print(check4)
      
      
      # Component Planes (Heat Maps) - Standardized Variable
      # Plot the component planes for each variable using a loop:
      ### NEW CODE: had to convert som_model$codes to a data frame in each call below
      par(mfrow = c(3, 3))
      par(cex = 0.6)
      par(mar = c(3, 3, 0, 0), oma = c(0, 0, 0, 0))
      #par(mgp = c(2, 0.5, 0))
      
      #par(mfrow=c(4,5), mai=c(0.2,0.4,0.2,0))  #mar = c(2,1,1,1), oma=c(0,0,0,0)
      for (c in 1:ncol(as.data.frame(som_model$codes))) {
            plot(som_model, type = "property", cex.lab = 1.4, border = "gray50", shape = "straight", property = as.data.frame(som_model$codes)[,c], main=colnames(as.data.frame(som_model$codes))[c], palette.name = coolBlueHotRed)
            add.cluster.boundaries(som_model, som_cluster)
      }
      
      # # # Component Planes (Heat Maps) - Unstandardized variable
      # ## NOTE: KU will need to add modification to this code from Shayne Lynn blog to address a 
      # # lattice that has empty nodes.
      # # intention is to map the average unscaled value (for a given variable) for 
      # # all input patterns mapped to each node
      # for (i in 1:ncol(som_model$codes)) {
      #       node_mns <- aggregate(as.numeric(myData_num[,i]), by=list(som_model$unit.classif), 
      #                                 FUN=mean, simplify=TRUE)[,2]
      #       #var_unscaled <- #need to fill in zero values for unoccupied nodes
      #       plot(som_model, type = "property", property=var_unscaled, main=colnames(myData_num)[i], palette.name=coolBlueHotRed)
      # }
      # 
      # # OR could try "reverse scaling" the codebook vectors stored in each node,
      # # by multiplying by the stdev of the original variables and
      # # adding the mean of the original variables
      # # perhaps there is a reverse function for scale() or I will need to calculate and store 
      # # stedev and mean in the early lines of the code for use here
      
      # Plot input patterns assigned to each node onto the clustered map
      par(mfrow=c(1,2), mar=c(6,3,3,1), oma=c(0,0,3,0))
      plot(som_model, type = "mapping" , shape = "straight", labels = som_model$unit.classif, main="Node ID Map (syst chk)") # maps node identification
      #plot(som_model, type = "mapping", labels = som_cluster, col=som_cluster+1, main="Mapping Plot")
      #add.cluster.boundaries(som_model, som_cluster)
      
      # check5 <- 'check5'
      # print(check5)      
      
      # Brittany didn't use the following chunck; commenting out for now
      ### Now match node numbers back to original [nrow] input patterns for plotting
      # newlabels <- cbind(som_model$unit.classif, observ, mySRClass)
      # plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], cex=0.75, main="Input Patterns \nMapped to each Node")
      # #plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], col = sedRegPallette[as.integer(mySRClass)], cex=0.75, main="Input Patterns \nMapped to each Node")
      # add.cluster.boundaries(som_model, som_cluster)
      
      ### Now match the cluster assignments back to the original [nrow] input patterns for plotting:
      # such as the line graphs with color coding for clusters that Peter Isles made
      # som_model$unit.classif # this is the node assignments for each data point
      # som_cluster # this is the cluster assignment for each node
      cluster_assign<-som_cluster[som_model$unit.classif] 
      # each data point, could be appended to the original dataset 
      cluster_assign
      
      # Now plot scaled variables by cluster as they differ from the mean
      par(mfrow=c(3,2), mar=c(7,3,3,1), oma=c(0,0,3,0))
      
      #p <- 1 #use this line to break loop for debugging
      for (p in 1:length(unique(cluster_assign))) {
            clusterData<- as.matrix(myData.sc[which(cluster_assign==p),], byrow = TRUE)
            if (nrow(clusterData) == 17) {
                  clusterData <- t(as.matrix(clusterData))
                  clusterMeans <- clusterData  # in case cluster has only one pattern
                  numPatterns <- 1
            } else {
                  clusterMeans<-apply(clusterData, 2, mean)
                  numPatterns <- nrow(clusterData)
            }
            
            dataMeans<-apply(myData.sc, 2, mean)  #compute overall mean for comparison
            barplot(clusterMeans-dataMeans, names.arg=names(clusterData), ylim = c(-.8,.8), las=3, main=paste("Cluster ", p, sep=""), col=barPalette[p])
            # barplot(clusterMeans-dataMeans, names.arg=names(clusterData), las=3, main=paste("Cluster ", p, sep=""), col=barPalette[p])
            mtext(paste("n =", numPatterns), side = 3, line = -0.5)
      }  #end for p loop  #ylim = c(-1.2, 1.2),
      
      #       # plot bar plots of variable values by cluster
      #       # bottom margin = 6.4 to leave room for x label with vertical hatch mark labels
      #       par(mfrow=c(3,2), mar=c(6.5,3,3,1), oma=c(0,0,3,0))
      #       for (pp in 1:length(unique(cluster_assign))) {
      #             clusterData<- as.matrix(myData.sc[which(cluster_assign==pp),], byrow = TRUE)
      #             if (nrow(clusterData) >1) {
      #                   numPatterns <- nrow(clusterData)
      #             } else {
      #                   clusterData <- t(as.matrix(clusterData)) # in case cluster has only one pattern
      #                   numPatterns <- 1
      #             }
      #             boxplot(clusterData ~ colnames(clusterData), xlab = "", ylab = "", 
      #             names=paste(colnames(clusterData)), las=2, cex=2, pch="*", col=barPalette[pp], 
      #             main=paste("Cluster ", pp, sep=""))
      #             mtext(paste("n =", numPatterns), side=3, line=0.1, at=0.0)
      # 
      #       }  #end for pp loop
      #       
      #repeat cluster plot for reference and using same color coding as bar charts
      par(mfrow=c(1,2), mar=c(6,3,3,1), oma=c(0,0,3,0))
      plot(som_model, type="mapping", shape = "straight", bgcol = barPalette[som_cluster], main = "Clusters")
      add.cluster.boundaries(som_model, som_cluster) 
      #mtext(paste("group mean - data mean for vars", normMeth), side=3, outer=TRUE)
      
      # Brittany did not use the next 2 chunks; commenting out for now
      #repeat plot of input patterns mapped to each node for ref next to cluster plot
      # plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], cex=0.75, main="Input Patterns \nMapped to each Node")
      # add.cluster.boundaries(som_model, som_cluster)
      
      #repeat large plot of input patterns mapped to each node for ref next to cluster plot
      # par(mfrow=c(1,1), mar=c(6,3,3,1), oma=c(0,0,3,0))
      # plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], col = sedRegPallette[as.integer(mySRClass)], cex=1.1, main="Input Patterns \nMapped to each Node \nColored by Mike Class")
      # add.cluster.boundaries(som_model, som_cluster)
      
      
      
      ###############
      #calculate nonparametric F statistic
      
      #first create dataframe of cluster assignments to call from the adonis2() function
      obsNum <- as.character(paste0("obs",observ))
      clustAssignDF <- as.data.frame(cbind(obsNum, cluster_assign))
      colnames(clustAssignDF) <- c('obsNum','Cluster') 
      #then run NP F statistic using adonis function of vegan package
      npFtest <- adonis2(obj.dists ~ Cluster, data=clustAssignDF, by = "terms", permutations=99)
      #extract F ratio and pVal for storage in results table
      npF <- npFtest$F[1]
      pVal_npF <- npFtest$`Pr(>F)`[1]
      
      # check6 <- 'check6'
      # print(check6)
      
      # pp <- 1 #use this line to break loop for debugging
      # #for (pp in 1:length(unique(cluster_assign))) {
      #   finalWghtsData<- as.matrix(myData.sc[which(cluster_assign==pp),], byrow = TRUE)
      #   if (nrow(clusterData) == 18) {
      #     clusterData <- t(as.matrix(clusterData))
      #     clusterMeans <- clusterData  # in case cluster has only one pattern
      #     numPatterns <- 1
      #   } else {
      #     clusterMeans<-apply(clusterData, 2, mean)
      #     numPatterns <- nrow(clusterData)
      #   }
      #   
      #   dataMeans<-apply(myData.sc, 2, mean)  #compute overall mean for comparison
      #   barplot(clusterMeans-dataMeans, ylim = c(-1, 2.5), names.arg=names(clusterData), las=3, main=paste("Cluster ", pp, sep=""), col=barPalette[pp])
      #   mtext(paste("n =", numPatterns), side = 3, line = -0.5)
      # #}  #end for pp loop
      
      
      dev.off()
      
      # ------------------- Track and Store results of each Run -------------------
      Results[i,14]<- mean(som_model$distances)
      Results[i,15] <- round(QE, digits = 6)
      Results[i,16] <- round(npF, digits = 6)
      Results[i,17] <- round(pVal_npF, digits = 6)
      
      clustAssignments <- rbind(clustAssignments, cluster_assign)
        
      
} # end for loop of SOM runs

# Print methods used
noquote(paste0("Topology: ", params[i,4]))
noquote(paste0("Data normalization method used: ", params[i,7]))
noquote(paste0("Weighting method used: ", params[i,8]))
noquote(paste0("No. of iterations: ", params[i,9]))
noquote(paste0("alphaCrs used: ", params[i,10]))
noquote(paste0("alphaFin used: ", params[i,9]))


# FORMAT & EXPORT RESULTS TABLES ----
colnames(Results) <- c('Run','DataSet','Nodes','Topol','rows','cols','normMeth', 
                       'wghtMeth','niter', 'alphaCrs', 'alphaFin', 
                       'Clusters', 'ColRowRat', 'Dist_mn','QE','npF','pVal_npF','blank') 

ResultsDF <- as_tibble(Results) %>% type_convert(col_types = cols(Run = col_double(), cols = col_double()))
write_csv(ResultsDF, here(paste0(newFolder,"/","Results_", myDataSet,"_",nclusters,"_cl",".csv")))

clustAssignm <- clustAssignments[-1, ]
colnames(clustAssignm) <- as.character(paste0("obs",observ))
# runID <- as.matrix(params[,1])
# colnames(runID)<- c("Run")
clustAssignm <- cbind(Results, clustAssignm)

clustAssignmDF <- as_tibble(clustAssignm)
write_csv(clustAssignmDF, here(paste0(newFolder,"/","ClustAssign_",myDataSet,"_",nclusters,"_cl",".csv")))
```
<br><br>

##### Choose the best SOM run based on non-parametric F-stat and quantization error
We want to maximize npF (ratio of b/w cluster variance) and minimize QE (mean distance b/w each data vector & best-matching unit)  
Here are the top 33% of runs based on npF
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.width = 8, out.width = "80%"}
# CHOOSE BEST LATTICE CONFIGURATION ----
  # Examine the non-parametric F-stat (npF) and quantization error (QE) to choose the best lattice configuration
  # You want to maximize npF (maximize b/w cluster variance while minimizing w/i cluster variance) &
  # Minimize QE (avg distance b/w each data vector & best-matching unit [BMU])

  # Set coefficient for second axis on plot below
  # Note: this might not be perfect and may need to adjust
  coeff <- median(ResultsDF$npF)/max(ResultsDF$QE)
  
  # Summarize the 3 replicate runs for each rowXcolXcluster combo
  # ResultsDF_summ <- 
  #   ResultsDF %>% 
  #   # Average and SEM of the 3 replicate runs for each rowXcolXcluster combo
  #   group_by(rows, cols, Nodes, Clusters) %>% 
  #   summarize(across(c(npF, QE), list(mean = ~mean(., na.rm = TRUE), sem = ~sd(., na.rm = TRUE)/sqrt(length(!is.na(.)))))) %>% 
  #   arrange(desc(npF_mean)) %>% 
  #   ungroup()

  # Plot npF and QE
  # ResultsDF_summ %>% 
  #   # Only show top 50% of choices
  #   slice(1:(nrow(.)/2)) %>% 
  #   # Create lattice config ID (rows x cols)
  #   mutate(latID = paste(paste(rows, "x", cols, sep = ""), "_", Clusters, "cl", sep = "")) %>% 
  #   # Plot
  #   ggplot(aes(x = reorder(latID, -npF_mean))) +
  #   geom_bar(aes(y = npF_mean, fill = "npF"), stat= "identity") +
  #   geom_errorbar(aes(ymax = npF_mean + npF_sem, ymin = npF_mean - npF_sem), width = 0.3) +
  #   geom_line(aes(y = QE_mean * coeff, group = 1, color = "QE")) +
  #   geom_point(aes(y = QE_mean * coeff, group = 1, color = "QE"), size = 2) +
  #   geom_errorbar(aes(ymax = QE_mean * coeff + QE_sem * coeff, ymin = QE_mean * coeff - QE_sem * coeff), color = "blue", width = 0.3) +
  #   scale_y_continuous(
  #     name = "npF",
  #     sec.axis = sec_axis(~. / coeff, name = "QE")
  #   ) +
  #   scale_fill_manual(values = "gray70") +
  #   scale_color_manual(values = "blue") +
  #   xlab("Lattice config. & clusters (rows x cols _ clusters)") +
  #   theme_bw() +
  #   theme(legend.title = element_blank(),
  #         legend.spacing = unit(-0.5, "lines"),
  #         panel.grid = element_blank(),
  #         axis.text.x = element_text(angle = 90, vjust = 0.5),
  #         axis.title.x = element_text(margin = margin(t = 5, r = 0, b = 0, l = 0)))
  
  ResultsDF %>% 
    # Sort by npF (max to min)
    arrange(desc(npF)) %>% 
    # Only show top 33% of choices
    slice(1:(nrow(.)/3)) %>% 
    # Create lattice config ID (rows x cols)
    mutate(latID = paste(paste(rows, "x", cols, sep = ""), "_", Clusters, "cl", "_", Run, sep = "")) %>% 
    # Plot
    ggplot(aes(x = reorder(latID, -npF))) +
    geom_bar(aes(y = npF, fill = "npF"), stat= "identity") +
    geom_line(aes(y = QE * coeff, group = 1, color = "QE")) +
    geom_point(aes(y = QE * coeff, group = 1, color = "QE"), size = 2) +
    scale_y_continuous(
      name = "npF",
      sec.axis = sec_axis(~. / coeff, name = "QE")
    ) +
    scale_fill_manual(values = "gray70") +
    scale_color_manual(values = "blue") +
    xlab("Lattice config. & clusters (rows x cols _ clusters_run)") +
    theme_bw() +
    theme(legend.title = element_blank(),
          legend.spacing = unit(-0.5, "lines"),
          panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90, vjust = 0.5),
          axis.title.x = element_text(margin = margin(t = 5, r = 0, b = 0, l = 0)))  
  
  # Save plot
  ggsave(here(newFolder, "npFvsQE.pdf"), device = "pdf", height = 4, width = 6, units = "in", dpi = 75)
```
<br><br>

##### Choose the best run from each high performing (top 33%) cluster #
```{r, include = TRUE, echo = FALSE, message = FALSE}
# Trying to automate the selection process
best_summ_top <- 
  ResultsDF %>% 
  select(Run, rows, cols, Nodes, Clusters, npF, QE) %>% 
  # Sort by npF (max to min)
  arrange(desc(npF)) %>% 
  # Only look at the first 30
  slice(1:30) %>% 
  # Arrange by QE (min to max)
  arrange(Clusters, QE) %>% 
  # Select the top (lowest QE) for each cluster #
  group_by(Clusters) %>% 
  slice(1:1) %>% 
  # Sort by npF (max to min)
  arrange(desc(npF)) %>% 
  ungroup()

noquote("These were the top runs for each high-performing (top 33%) cluster #:")
best_summ_top %>% 
  mutate_at(vars(c(npF, QE)),
            .funs = ~round(., 3)) %>% 
  gt()

# Select the top one
best_summ_top1 <-
  best_summ_top %>% 
  slice(1:1)

noquote("The script chose:")
best_summ_top1 %>% 
  mutate_at(vars(c(npF, QE)),
            .funs = ~round(., 3)) %>%   
  gt()

# Choose lattice dimensions you want to examine based on npf/QE plot above
  n_rows = best_summ_top1$rows
  n_cols = best_summ_top1$cols
  n_clust = best_summ_top1$Clusters
  
# Look at the reps of this combo
# best <- ResultsDF %>% 
#   filter(rows == n_rows & cols == n_cols & Clusters == n_clust) %>% 
#   select(Run, Nodes, rows, cols, npF, QE) %>% 
#   arrange(desc(npF))
# print(best)

# Which Run is best?
  best_run = best_summ_top1$Run


# CREATE DF WITH EVENT IDs & CLUSTER #'s ----  
datWithCluster <- clustAssignmDF %>% 
  # Parse columns into correct type
  type_convert() %>% 
  filter(Run == best_run) %>% 
  # Pivot to longer format
  pivot_longer(cols = obs1:ncol(.), names_to = "obs", values_to = "cluster") %>% 
  select(cluster) %>% 
  # Bind cluster ID to myData
  bind_cols(myData) %>% 
  # Join to original data to get event IDs
  left_join(hford) %>% 
  mutate(event_start = ymd_hms(event_start, tz = "Etc/GMT+4")) %>% 
  # Drop all the independent variables you didn't use in the SOM (copy from myData above); 
  # but KEEP the INFO vars
  select(-one_of(drop.vars)) %>% 
  # Arrange columns
  select(site:ncol(.), everything())
  
  # Write to CSV
  datWithCluster %>% 
    mutate(event_start = as.character(event_start)) %>% 
    write_csv(here(paste0(newFolder, "/", "SOMresults", "_", myDataSet, "_", Sys.Date(), "_", nclusters, "cl", "_", n_rows, "x", n_cols, ".csv")))
```
<br><br>

To examine this run in greater detail (e.g., component planes), see the 'X_SOMplots_site_ ... .pdf'  
<br><br>

##### Examine boxplots of independent variables by cluster
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.4}
# BOXPLOTS OF INDEPENDENT VARIABLES BY CLUSTER ----
  # # of plots (use this to decide how many PDF pages you want to plot all the plots on)
  df <- datWithCluster %>% 
    pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value")
  # length(unique(df$var))

  datWithCluster %>% 
    pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
    ggplot(aes(x = cluster, y = value, group = cluster)) +
    facet_wrap(~var, scales = "free_y", ncol = 4) +
    geom_boxplot(fill = "white") +
    geom_point(position=position_jitter(width=0.2), shape=1, size=1, color="gray50", alpha=0.6) +
    ylab("Value") + xlab("Cluster") +
    theme_bw() +
    theme(panel.grid = element_blank(),
          strip.text = element_text(size = 7),
          panel.spacing.x = unit(0.3, "inches"),
          panel.spacing.y = unit(0.45, "inches"))
```
<br><br>

```{r pdfs of boxplots, include = FALSE}
# Plot & save the boxplots created above
pdf(here(paste0(newFolder, "/", "boxplots_varsByCluster", "_", n_rows, "x", n_cols, ".pdf")),
    width = 7.5, height = 10)
for(i in 1:2){ # this is the # of PDF pages your plots will be spread across
  print(datWithCluster %>% 
    pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
    ggplot(aes(x = cluster, y = value, group = cluster)) +
    facet_wrap_paginate(~var, scales = "free_y", ncol = 4, nrow = 3, page = i) +
    geom_boxplot(fill = "white") +
    geom_point(position=position_jitter(width=0.2), shape=1, size=1, color="gray50", alpha=0.6) +
    theme_bw() +
    theme(panel.grid = element_blank(),
          strip.text = element_text(size = 7),
          panel.spacing.x = unit(0.3, "inches"),
          panel.spacing.y = unit(0.45, "inches")))
}
dev.off()
```

```{r plot themes and labels, include = FALSE}
# Plotting specifics
  theme1 <- theme_classic() +
            theme(axis.text = element_text(size = 11),
                  axis.title = element_text(size = 12),
                  axis.title.x = element_text(margin=margin(5,0,0,0)),
                  axis.title.y = element_text(margin=margin(0,5,0,0)),
                  legend.title = element_text(size = 9),
                  legend.text = element_text(size = 9))
  
  # A new theme for the hysteresis plots
  theme3 <- theme(axis.line = element_blank(),
                  axis.text = element_text(size = 8),
                  axis.title = element_blank(),
                  plot.margin = unit(c(1.25,1.25,1.25,1.25), "lines"),
                  panel.background = element_blank(),
                  panel.grid.major = element_blank(), 
                  panel.grid.minor = element_blank(),
                  legend.text = element_text(size = 6),
                  legend.key.size = unit(0.1, "in"),
                  legend.margin = margin(0, 0, 0, 0),
                  legend.title = element_text(size = 6)) 
```

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1}
# Look at z-scores by cluster
# Calculate median z-scores
# datWithCluster_z <- datWithCluster %>% 
#   # Rename clusters
#   mutate(cluster = paste("Cluster", cluster, sep = " ")) %>% 
#   # Calculate z-scores for each independent variable
#   mutate_at(vars(c(DOY:ncol(.))),
#           .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
#   pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
#   group_by(cluster, var) %>% 
#   summarize(median = median(value, na.rm = T)) %>% 
#   # Order the vars n var
#   mutate(var = factor(var, levels = c("turb_event_max", "multipeak", "q_mm", "rain_event_total_mm", "rain_event_hrs", "rain_int_mmPERmin_mean", "turb_1d", "SRP_1d", "NO3_1d",                                            "q_4d", "time_sinceLastEvent", "API_4d", "Redox_pre_wet_15cm", "DO_pre_wet_15cm", "VWC_pre_wet_15cm", "SoilTemp_pre_wet_15cm", 
#                                       "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY")))



# Trying to change x-axis label colors; doesn't work
# Help here: https://stackoverflow.com/questions/38862303/customize-ggplot2-axis-labels-with-different-colors
# colors_xlabels <- ifelse(datWithCluster_z$var %in% c("solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY"), "#009E73", 
#                          ifelse(datWithCluster_z$var %in% c("Redox_pre_wet_15cm", "DO_pre_wet_15cm", "VWC_pre_wet_15cm", "SoilTemp_pre_wet_15cm"), "#E69F00",
#                                 ifelse(datWithCluster_z$var %in% c("time_sinceLastEvent", "API_4d"), "#000000",
#                                        ifelse(datWithCluster_z$var %in% c("turb_1d", "SRP_1d", "NO3_1d", "q_4d"), "#56B4E9",
#                                               ifelse(datWithCluster_z$var %in% c("q_mm", "rain_event_total_mm", "rain_event_hrs", "rain_int_mmPERmin_mean"), "#0072B2", "#CC79A7")))))


datWithCluster %>% 
  # Rename clusters
  mutate(cluster = paste("Cluster", cluster, sep = " ")) %>% 
  # Calculate z-scores for each independent variable
  mutate_at(vars(c(DOY:ncol(.))),
          .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
  group_by(cluster, var) %>% 
  summarize(median = median(value, na.rm = T)) %>% 
  # Order the vars n var
  mutate(var = factor(var, levels = c("turb_event_max", "multipeak", "q_mm", "rain_event_total_mm", "rain_event_hrs", "rain_int_mmPERmin_mean", "SRP_1d", "NO3_1d",
                                      "q_4d", "time_sinceLastEvent", "API_4d", "Redox_pre_wet_15cm", "DO_pre_wet_15cm", "VWC_pre_wet_15cm", "SoilTemp_pre_wet_15cm", 
                                      "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY"))) %>% 
  ggplot(aes(x = var, y = median)) +
    # Must use scales = "free" when using the reordering functions
    facet_wrap(~cluster, ncol = 2) +
    geom_bar(stat = "identity") +
    # scale_x_reordered() +
    ylab("Median z-scores") +
    coord_flip() +
    theme_bw() +
    theme(panel.grid = element_blank(),
          axis.title.y = element_blank()) +
  ggtitle("Characteristics of each cluster - median z-scores")

ggsave(here("Plots", "SOM_hf_zScoresByCluster.pdf"), device = "pdf", width = 6, height = 6, units = "in", dpi = 150)

# Plotting mean z-scores + SE
datWithCluster %>% 
  # Rename clusters
  mutate(cluster = paste("Cluster", cluster, sep = " ")) %>% 
  # Calculate z-scores for each independent variable
  mutate_at(vars(c(DOY:ncol(.))),
          .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
  group_by(cluster, var) %>% 
  summarize(mean = mean(value),
            SE = sd(value)/sqrt(length(!is.na(value)))) %>% 
  # Order the vars n var
  mutate(var = factor(var, levels = c("turb_event_max", "multipeak", "q_mm", "rain_event_total_mm", "rain_event_hrs", "rain_int_mmPERmin_mean", "SRP_1d", "NO3_1d",
                                      "q_4d", "time_sinceLastEvent", "API_4d", "Redox_pre_wet_15cm", "DO_pre_wet_15cm", "VWC_pre_wet_15cm", "SoilTemp_pre_wet_15cm", 
                                      "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY"))) %>%   
  ggplot(aes(x = var, y = mean)) +
    # Must use scales = "free" when using the reordering functions
    facet_wrap(~cluster, ncol = 2) +
    geom_bar(stat = "identity", position = position_dodge()) +
    geom_errorbar(aes(ymin = mean - SE, ymax = mean + SE), width = 0.2, position = position_dodge(0.9)) +
    # scale_x_reordered() +
    ylab("Median z-scores") +
    coord_flip() +
    theme_bw() +
    theme(panel.grid = element_blank(),
          axis.title.y = element_blank()) +
  ggtitle("Characteristics of each cluster - mean z-scores +/- SEM")
```
<br><br>

#### Examine how antecedent and event conditions converge to influence N & P flux regimes:
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Look at seasonal differences by cluster
# datWithCluster %>% 
#   group_by(cluster, season) %>% 
#   tally() %>% 
#   # Order the seasons
#   mutate(season = factor(season, levels = c("spring", "summer", "fall"))) %>%
#   ggplot(aes(x = cluster, y = n, fill = season)) +
#     geom_bar(position = "stack", stat = "identity") +
#     scale_fill_manual(name = "Season",
#                       breaks = c("spring", "summer", "fall"),
#                       labels = c("Spring", "Summer", "Fall"),
#                       values = c("#0072B2", "#009E73", "#E69F00")) +
#     ylab("No. of events") +
#     xlab("Cluster") +
#     theme1 +
#     ggtitle("")
# 
# ggsave("../Plots/SOM_hf_seasonsByClusters.png", width = 4, height = 4, units = "in", dpi = 150)

datWithCluster %>% 
  group_by(cluster, season) %>% 
  tally() %>% 
  # Order the seasons
  mutate(season = factor(season, levels = c("spring", "summer", "fall"), labels = c("Spring", "Summer", "Fall"))) %>%
  mutate(cluster = factor(cluster)) %>% 
  ggplot(aes(x = season, y = n, fill = cluster)) +
    geom_bar(position = "stack", stat = "identity") +
    scale_fill_manual(name = "Cluster",
                      values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
    ylab("No. of events") +
    xlab("Season") +
    theme1

ggsave(here("Plots", "SOM_hf_clustersBySeason.pdf"), device = "pdf", width = 4, height = 4, units = "in", dpi = 150)
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, out.width = "80%"}
# Plot timeline of event yields & ratios colored by cluster number
datWithCluster %>% 
  # Only 2 events in 2017, so cutting out 2017
  filter(year(event_start) != 2017) %>% 
   # Add leading zero to single digit months and days
  mutate(year = year(event_start),
         month = str_pad(month(event_start), 2, pad = 0),
         day = str_pad(day(event_start), 2, pad = 0),
         month_day = paste(month, "/", day, sep = "")) %>%
  # There is one day where there are two events, add a rep for month_day and make month_day_rep ID
  group_by(year, month_day) %>% 
  mutate(rep = row_number()) %>% 
  mutate(month_day_rep = paste(month_day, rep, sep = "-")) %>% 
  # Calculate log of NO3:SRP yield ration
  mutate(log_event_NO3_SRP = log(event_NO3_SRP)) %>% 
  pivot_longer(cols = c(NO3_kg_km2, SRP_kg_km2, log_event_NO3_SRP), names_to = "var", values_to = "value") %>% 
  mutate(var = factor(var, levels = c("NO3_kg_km2", "SRP_kg_km2", "log_event_NO3_SRP"))) %>% 
  
  ggplot(aes(x = month_day_rep, y = value, fill = as.factor(cluster))) +
    # facet_wrap(var ~ year, scales = "free", ncol = 2) +
    facet_grid(var ~ year, scales = "free") +
    geom_bar(stat = "identity") +
    scale_fill_manual(name = "Cluster",
                      values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
    ylab(expression(Yield~(kg~N~or~P~km^{-2})~or~yield~ratio)) +
    xlab("Date (month/day - #)") +
    theme_bw() +
    theme(panel.grid = element_blank(),
          strip.text.x = element_text(size = 8),
          axis.text.x = element_text(angle = 90))

ggsave(here("Plots", "SOM_hf_eventsByCluster.pdf"), device = "pdf", width = 10, height = 6, units = "in", dpi = 150)
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Plot NO3 & SRP yields against water yield
datWithCluster %>% 
  pivot_longer(cols = c(NO3_kg_km2, SRP_kg_km2), names_to = "var", values_to = "value") %>% 
  ggplot(aes(x = q_mm, y = value)) +
    facet_wrap(~var, ncol = 1, scales = "free_y") +
    geom_point(aes(color = as.factor(cluster)), size = 2, stroke = 0.75, alpha = 0.8) +
    geom_smooth(method=lm, se=TRUE) +
    scale_color_manual(name = "Cluster",
                      values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
    ylab(expression(Event~NO[3]^-{}~or~SRP~yield~(kg~N~or~P~km^{-2}))) +
    xlab("Event water yield (mm)") +
    # scale_y_continuous(limits = c(0, 1300), breaks=seq(0, 1200, 300)) +
    # facet_wrap(~site, labeller = labeller(var = labels, site = labels2)) +
    # theme1
    theme_classic() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          strip.background = element_blank())
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Plot NO3:SRP yield ratios against water yield
datWithCluster %>% 
  ggplot(aes(x = q_mm, y = log(event_NO3_SRP), color = as.factor(cluster))) +
    geom_point(size = 2, stroke = 0.75, alpha = 0.8) +
    scale_color_manual(name = "Cluster",
                      values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
    # geom_hline(yintercept = 16, linetype = "dashed", size = 0.75, color = "gray40") +
    # ylab(expression(paste(Ratio~of~"NO"["3"]^" -"~" : "~SRP~event~yield))) + 
    ylab(expression(atop(log~Molar~ratio~of~NO[3]^-{}~":"~SRP~yield, "for"~each~event))) +
    xlab("Event water yield (mm)") +
    # scale_y_continuous(limits = c(0, 1300), breaks=seq(0, 1200, 300)) +
    # facet_wrap(~site, labeller = labeller(var = labels, site = labels2)) +
    theme1

ggsave(here("Plots", "SOM_hf_ratiosBYWaterYield.pdf"), device = "pdf",  width = 4, height = 4, units = "in", dpi = 150)
```
<br><br>

```{r, include = FALSE}
# Text for Clockwise, Counterclockwise, Diluting, Flushing on the plots
text_cw <- textGrob("Clockwise", gp = gpar(fontsize = 10))
text_ccw <- textGrob("Counterclockwise", gp = gpar(fontsize = 10))
text_dil <- textGrob("Diluting", gp = gpar(fontsize = 10), rot = 90)
text_flu <- textGrob("Flushing", gp = gpar(fontsize = 10), rot = 270)

# Nitrate
pl_hifi_no3 <- 
  datWithCluster %>% 
  left_join(hyst %>% filter(site == "Hungerford"), by = c("site", "event_start")) %>%
  ggplot() +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_point(aes(x = FI_NO3, y = HI_NO3_mean, color = as.factor(cluster)), size = 2, alpha = 0.8) +
  scale_color_manual(name = "Cluster",
                     values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
  ylim(-1, 1) + xlim(-1, 1) +
  ylab("Storm hysteresis index") + xlab("Storm flushing index") +
  theme3 +
  ggtitle("Nitrate") + 
  theme(plot.title = element_text(hjust = 0, size = 12),
        legend.position = c(0.8, 0.85),
        legend.box = "horizontal") +
  annotation_custom(text_dil, xmin=-1.45, xmax=-1.45, ymin=0, ymax=0) +
  annotation_custom(text_flu, xmin=1.2, xmax=1.2, ymin=0, ymax=0) +
  annotation_custom(text_cw, xmin=0, xmax=0, ymin=1.2, ymax=1.2) +
  annotation_custom(text_ccw, xmin=0, xmax=0, ymin=-1.4, ymax=-1.4) +
  coord_cartesian(clip = "off")

# SRP
pl_hifi_srp <- 
  datWithCluster %>% 
  left_join(hyst %>% filter(site == "Hungerford"), by = c("site", "event_start")) %>%
  ggplot() +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_point(aes(x = FI_SRP, y = HI_SRP_mean, color = as.factor(cluster)), size = 2, alpha = 0.8) +
  scale_color_manual(name = "Cluster",
                     values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
  ylim(-1, 1) + xlim(-1, 1) +
  ylab("Storm hysteresis index") + xlab("Storm flushing index") +
  theme3 +
  ggtitle("SRP") + 
  theme(plot.title = element_text(hjust = 0, size = 12),
        legend.position = "none") +
  annotation_custom(text_dil, xmin=-1.45, xmax=-1.45, ymin=0, ymax=0) +
  annotation_custom(text_flu, xmin=1.2, xmax=1.2, ymin=0, ymax=0) +
  annotation_custom(text_cw, xmin=0, xmax=0, ymin=1.2, ymax=1.2) +
  annotation_custom(text_ccw, xmin=0, xmax=0, ymin=-1.4, ymax=-1.4) +
  coord_cartesian(clip = "off")

# Combine the 2 plots into one
pl_HI_v_FI_alt <- plot_grid(pl_hifi_no3, pl_hifi_srp, ncol = 2, align = "hv",
                            labels = "auto", hjust = -0.4, vjust = 2.5, scale = 1)
# Create common x and y axis titles
y.grob <- textGrob("Storm hysteresis index",
                   gp = gpar(fontsize = 13), rot = 90)
x.grob <- textGrob("Storm flushing index",
                   gp = gpar(fontsize = 13))
# Add axis title to plot
# test_alt <- grid.arrange(arrangeGrob(pl_HI_v_FI_alt, left = y.grob, bottom = x.grob))
# ggsave("Plots/HIFI_hf.png", test_alt, width = 6, height = 3.5, units = "in", dpi = 300)     
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Add axis title to plot
# grid.arrange(arrangeGrob(pl_HI_v_FI_alt, left = y.grob, bottom = x.grob))
test_alt <- grid.arrange(arrangeGrob(pl_HI_v_FI_alt, left = y.grob, bottom = x.grob))
ggsave(here("Plots", "HIFI_hf.pdf"), plot = test_alt, device = "pdf", width = 6, height = 3.5, units = "in", dpi = 300)     
```
<br><br>

#### How do our results differ if we choose the 2nd best SOM run?
```{r, include = TRUE, echo = FALSE, message = FALSE}
# Select the top one
best_summ_2ndBest <-
  best_summ_top %>% 
  slice(2:2)

noquote("The 2nd best run was:")
best_summ_2ndBest %>% 
  mutate_at(vars(c(npF, QE)),
            .funs = ~round(., 3)) %>%     
  gt()

# Choose lattice dimensions you want to examine based on npf/QE plot above
  n_rows_2 = best_summ_2ndBest$rows
  n_cols_2 = best_summ_2ndBest$cols
  n_clust_2 = best_summ_2ndBest$Clusters

# Which Run is best?
  best_run_2 = best_summ_2ndBest$Run

# CREATE DF WITH EVENT IDs & CLUSTER #'s ----  
datWithCluster2 <- clustAssignmDF %>% 
  # Parse columns into correct type
  type_convert() %>% 
  filter(Run == best_run_2) %>% 
  # Pivot to longer format
  pivot_longer(cols = obs1:ncol(.), names_to = "obs", values_to = "cluster") %>% 
  select(cluster) %>% 
  # Bind cluster ID to myData
  bind_cols(myData) %>% 
  # Join to original data to get event IDs
  left_join(hford) %>% 
  mutate(event_start = ymd_hms(event_start, tz = "Etc/GMT+4")) %>% 
  # Drop all the independent variables you didn't use in the SOM (copy from myData above); 
  # but KEEP the INFO vars
  select(-one_of(drop.vars)) %>% 
  # Arrange columns
  select(site:ncol(.), everything())
  
  # Write to CSV
  # datWithCluster %>% 
  #   mutate(event_start = as.character(event_start)) %>% 
  #   write_csv(here(paste0(newFolder, "/", "SOMresults", "_", myDataSet, "_", Sys.Date(), "_", nclusters, "cl", "_", n_rows, "x", n_cols, ".csv")))
```
<br><br>

To examine this run in greater detail (e.g., component planes), see the 'X_SOMplots_site_ ... .pdf'  
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.4}
# BOXPLOTS OF INDEPENDENT VARIABLES BY CLUSTER ----
  # # of plots (use this to decide how many PDF pages you want to plot all the plots on)
  df2 <- datWithCluster2 %>% 
    pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value")
  # length(unique(df$var))

  datWithCluster2 %>% 
    pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
    ggplot(aes(x = cluster, y = value, group = cluster)) +
    facet_wrap(~var, scales = "free_y", ncol = 4) +
    geom_boxplot(fill = "white") +
    geom_point(position=position_jitter(width=0.2), shape=1, size=1, color="gray50", alpha=0.6) +
    ylab("Value") + xlab("Cluster") +
    theme_bw() +
    theme(panel.grid = element_blank(),
          strip.text = element_text(size = 7),
          panel.spacing.x = unit(0.3, "inches"),
          panel.spacing.y = unit(0.45, "inches"))
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1}
# Look at z-scores by cluster
datWithCluster2 %>% 
  # Rename clusters
  mutate(cluster = paste("Cluster", cluster, sep = " ")) %>% 
  # Calculate z-scores for each independent variable
  mutate_at(vars(c(DOY:ncol(.))),
          .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
  group_by(cluster, var) %>% 
  summarize(median = median(value, na.rm = T)) %>% 
  # Order the vars n var
  mutate(var = factor(var, levels = c("turb_event_max", "multipeak", "q_mm", "rain_event_total_mm", "rain_event_hrs", "rain_int_mmPERmin_mean", "SRP_1d", "NO3_1d",
                                      "q_4d", "time_sinceLastEvent", "API_4d", "Redox_pre_wet_15cm", "DO_pre_wet_15cm", "VWC_pre_wet_15cm", "SoilTemp_pre_wet_15cm", 
                                      "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY"))) %>% 
  ggplot(aes(x = var, y = median)) +
    # Must use scales = "free" when using the reordering functions
    facet_wrap(~cluster, ncol = 2) +
    geom_bar(stat = "identity") +
    # scale_x_reordered() +
    ylab("Median z-scores") +
    coord_flip() +
    theme_bw() +
    theme(panel.grid = element_blank(),
          axis.title.y = element_blank()) +
  ggtitle("Characteristics of each cluster - median z-scores")

# ggsave(here("Plots", "SOM_hf_zScoresByCluster.pdf"), device = "pdf", width = 6, height = 6, units = "in", dpi = 150)

# Plotting mean z-scores + SE
datWithCluster2 %>% 
  # Rename clusters
  mutate(cluster = paste("Cluster", cluster, sep = " ")) %>% 
  # Calculate z-scores for each independent variable
  mutate_at(vars(c(DOY:ncol(.))),
          .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  pivot_longer(cols = DOY:ncol(.), names_to = "var", values_to = "value") %>% 
  group_by(cluster, var) %>% 
  summarize(mean = mean(value),
            SE = sd(value)/sqrt(length(!is.na(value)))) %>% 
  # Order the vars n var
  mutate(var = factor(var, levels = c("turb_event_max", "multipeak", "q_mm", "rain_event_total_mm", "rain_event_hrs", "rain_int_mmPERmin_mean", "SRP_1d", "NO3_1d",
                                      "q_4d", "time_sinceLastEvent", "API_4d", "Redox_pre_wet_15cm", "DO_pre_wet_15cm", "VWC_pre_wet_15cm", "SoilTemp_pre_wet_15cm", 
                                      "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY"))) %>%   
  ggplot(aes(x = var, y = mean)) +
    # Must use scales = "free" when using the reordering functions
    facet_wrap(~cluster, ncol = 2) +
    geom_bar(stat = "identity", position = position_dodge()) +
    geom_errorbar(aes(ymin = mean - SE, ymax = mean + SE), width = 0.2, position = position_dodge(0.9)) +
    # scale_x_reordered() +
    ylab("Median z-scores") +
    coord_flip() +
    theme_bw() +
    theme(panel.grid = element_blank(),
          axis.title.y = element_blank()) +
  ggtitle("Characteristics of each cluster - mean z-scores +/- SEM")
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
datWithCluster2 %>% 
  group_by(cluster, season) %>% 
  tally() %>% 
  # Order the seasons
  mutate(season = factor(season, levels = c("spring", "summer", "fall"), labels = c("Spring", "Summer", "Fall"))) %>%
  mutate(cluster = factor(cluster)) %>% 
  ggplot(aes(x = season, y = n, fill = cluster)) +
    geom_bar(position = "stack", stat = "identity") +
    scale_fill_manual(name = "Cluster",
                      values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
    ylab("No. of events") +
    xlab("Season") +
    theme1

# ggsave(here("Plots", "SOM_hf_clustersBySeason.pdf"), device = "pdf", width = 4, height = 4, units = "in", dpi = 150)
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, out.width = "80%"}
# Plot timeline of event yields & ratios colored by cluster number
datWithCluster2 %>% 
  # Only 2 events in 2017, so cutting out 2017
  filter(year(event_start) != 2017) %>% 
   # Add leading zero to single digit months and days
  mutate(year = year(event_start),
         month = str_pad(month(event_start), 2, pad = 0),
         day = str_pad(day(event_start), 2, pad = 0),
         month_day = paste(month, "/", day, sep = "")) %>%
  # There is one day where there are two events, add a rep for month_day and make month_day_rep ID
  group_by(year, month_day) %>% 
  mutate(rep = row_number()) %>% 
  mutate(month_day_rep = paste(month_day, rep, sep = "-")) %>% 
  # Calculate log of NO3:SRP yield ration
  mutate(log_event_NO3_SRP = log(event_NO3_SRP)) %>% 
  pivot_longer(cols = c(NO3_kg_km2, SRP_kg_km2, log_event_NO3_SRP), names_to = "var", values_to = "value") %>% 
  mutate(var = factor(var, levels = c("NO3_kg_km2", "SRP_kg_km2", "log_event_NO3_SRP"))) %>% 
  
  ggplot(aes(x = month_day_rep, y = value, fill = as.factor(cluster))) +
    # facet_wrap(var ~ year, scales = "free", ncol = 2) +
    facet_grid(var ~ year, scales = "free") +
    geom_bar(stat = "identity") +
    scale_fill_manual(name = "Cluster",
                      values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
    ylab(expression(Yield~(kg~N~or~P~km^{-2})~or~yield~ratio)) +
    xlab("Date (month/day - #)") +
    theme_bw() +
    theme(panel.grid = element_blank(),
          strip.text.x = element_text(size = 8),
          axis.text.x = element_text(angle = 90))

# ggsave(here("Plots", "SOM_hf_eventsByCluster.pdf"), device = "pdf", width = 10, height = 6, units = "in", dpi = 150)
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Plot NO3 & SRP yields against water yield
datWithCluster2 %>% 
  pivot_longer(cols = c(NO3_kg_km2, SRP_kg_km2), names_to = "var", values_to = "value") %>% 
  ggplot(aes(x = q_mm, y = value)) +
    facet_wrap(~var, ncol = 1, scales = "free_y") +
    geom_point(aes(color = as.factor(cluster)), size = 2, stroke = 0.75, alpha = 0.8) +
    geom_smooth(method=lm, se=TRUE) +
    scale_color_manual(name = "Cluster",
                      values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
    ylab(expression(Event~NO[3]^-{}~or~SRP~yield~(kg~N~or~P~km^{-2}))) +
    xlab("Event water yield (mm)") +
    # scale_y_continuous(limits = c(0, 1300), breaks=seq(0, 1200, 300)) +
    # facet_wrap(~site, labeller = labeller(var = labels, site = labels2)) +
    # theme1
    theme_classic() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          strip.background = element_blank())
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Plot NO3:SRP yield ratios against water yield
datWithCluster2 %>% 
  ggplot(aes(x = q_mm, y = log(event_NO3_SRP), color = as.factor(cluster))) +
    geom_point(size = 2, stroke = 0.75, alpha = 0.8) +
    scale_color_manual(name = "Cluster",
                      values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
    # geom_hline(yintercept = 16, linetype = "dashed", size = 0.75, color = "gray40") +
    # ylab(expression(paste(Ratio~of~"NO"["3"]^" -"~" : "~SRP~event~yield))) + 
    ylab(expression(atop(log~Molar~ratio~of~NO[3]^-{}~":"~SRP~yield, "for"~each~event))) +
    xlab("Event water yield (mm)") +
    # scale_y_continuous(limits = c(0, 1300), breaks=seq(0, 1200, 300)) +
    # facet_wrap(~site, labeller = labeller(var = labels, site = labels2)) +
    theme1

# ggsave(here("Plots", "SOM_hf_ratiosBYWaterYield.pdf"), device = "pdf",  width = 4, height = 4, units = "in", dpi = 150)
```
<br><br>

```{r, include = FALSE}
# Nitrate
pl_hifi_no3_2 <- 
  datWithCluster2 %>% 
  left_join(hyst %>% filter(site == "Hungerford"), by = c("site", "event_start")) %>%
  ggplot() +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_point(aes(x = FI_NO3, y = HI_NO3_mean, color = as.factor(cluster)), size = 2, alpha = 0.8) +
  scale_color_manual(name = "Cluster",
                     values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
  ylim(-1, 1) + xlim(-1, 1) +
  ylab("Storm hysteresis index") + xlab("Storm flushing index") +
  theme3 +
  ggtitle("Nitrate") + 
  theme(plot.title = element_text(hjust = 0, size = 12),
        legend.position = c(0.8, 0.85),
        legend.box = "horizontal") +
  annotation_custom(text_dil, xmin=-1.45, xmax=-1.45, ymin=0, ymax=0) +
  annotation_custom(text_flu, xmin=1.2, xmax=1.2, ymin=0, ymax=0) +
  annotation_custom(text_cw, xmin=0, xmax=0, ymin=1.2, ymax=1.2) +
  annotation_custom(text_ccw, xmin=0, xmax=0, ymin=-1.4, ymax=-1.4) +
  coord_cartesian(clip = "off")

# SRP
pl_hifi_srp_2 <- 
  datWithCluster2 %>% 
  left_join(hyst %>% filter(site == "Hungerford"), by = c("site", "event_start")) %>%
  ggplot() +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_point(aes(x = FI_SRP, y = HI_SRP_mean, color = as.factor(cluster)), size = 2, alpha = 0.8) +
  scale_color_manual(name = "Cluster",
                     values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
  ylim(-1, 1) + xlim(-1, 1) +
  ylab("Storm hysteresis index") + xlab("Storm flushing index") +
  theme3 +
  ggtitle("SRP") + 
  theme(plot.title = element_text(hjust = 0, size = 12),
        legend.position = "none") +
  annotation_custom(text_dil, xmin=-1.45, xmax=-1.45, ymin=0, ymax=0) +
  annotation_custom(text_flu, xmin=1.2, xmax=1.2, ymin=0, ymax=0) +
  annotation_custom(text_cw, xmin=0, xmax=0, ymin=1.2, ymax=1.2) +
  annotation_custom(text_ccw, xmin=0, xmax=0, ymin=-1.4, ymax=-1.4) +
  coord_cartesian(clip = "off")

# Combine the 2 plots into one
pl_HI_v_FI_alt_2 <- plot_grid(pl_hifi_no3_2, pl_hifi_srp_2, ncol = 2, align = "hv",
                            labels = "auto", hjust = -0.4, vjust = 2.5, scale = 1)
# Create common x and y axis titles
y.grob <- textGrob("Storm hysteresis index",
                   gp = gpar(fontsize = 13), rot = 90)
x.grob <- textGrob("Storm flushing index",
                   gp = gpar(fontsize = 13))
# Add axis title to plot
# test_alt <- grid.arrange(arrangeGrob(pl_HI_v_FI_alt, left = y.grob, bottom = x.grob))
# ggsave("Plots/HIFI_hf.png", test_alt, width = 6, height = 3.5, units = "in", dpi = 300)     
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Add axis title to plot
# grid.arrange(arrangeGrob(pl_HI_v_FI_alt, left = y.grob, bottom = x.grob))
test_alt_2 <- grid.arrange(arrangeGrob(pl_HI_v_FI_alt_2, left = y.grob, bottom = x.grob))
# ggsave(here("Plots", "HIFI_hf.pdf"), plot = test_alt, device = "pdf", width = 6, height = 3.5, units = "in", dpi = 300)     
```
---
title: "SOM analysis (PCA to SOM)"
author: "Dustin Kincaid"
date: "4/28/2020<br><br>"
output: html_notebook
---

```{r, include=FALSE}
# A helpful website about Rmarkdown
# https://ourcodingclub.github.io/tutorials/rmarkdown/
```

```{r, include=FALSE, warning=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("lubridate")
library("rcompanion")
library("factoextra")
library("cowplot")
```

```{r, include = FALSE, warning = FALSE}
# Read in calculated event metrics from compile_calculate_allVars.R
hford <- read_csv("../Data/eventMetrics_hford.csv", col_types = cols()) %>% 
  # Shorten column names
  rename(r_e_mm = rain_event_total_mm, r_e_hrs = rain_event_hrs, r_e_intMax = rain_int_mmPERmin_max, r_e_intMean = rain_int_mmPERmin_mean, 
         r_pre1d = rain_preEvent_1d, r_pre7d = rain_preEvent_7d, r_pre14d = rain_preEvent_14d, r_pre30d = rain_preEvent_30d, 
         q_e_max = q_event_max, q_e_del = q_event_delta, q_e_delQrt = q_event_dQRate_cmsPerHr, q_pre_1d = q_preEvent_mean_1d, 
         q_pre_7d = q_preEvent_mean_7d, NO3 = NO3_kg_km2, SRP = SRP_kg_km2, NO3toSRP = event_NO3_SRP, turb = turb_kg_km2, turb_max = turb_event_max,
         gw_e_max = gw_event_max_well5, gw_e_del = gw_event_delta_well5, gw_pre_1d = gw_preEvent_mean_1d_well5, gw_pre_7d = gw_preEvent_mean_7d_well5,
         TSLE = time_sinceLastEvent, PET = PET_mmHR, DO_p_up = DO_mean_preEvent_1, RED_pre_up = Redox_mean_preEvent_1, 
         ST_pre_up = SoilTemp_mean_preEvent_1, VWC_pre_up = VWC_mean_preEvent_1, DO_p_low = DO_mean_preEvent_3, RED_pre_low = Redox_mean_preEvent_3,
         ST_pre_low = SoilTemp_mean_preEvent_3, VWC_pre_low = VWC_mean_preEvent_3)
wade <- read_csv("../Data/eventMetrics_wade.csv", col_types = cols()) %>% 
  rename(r_e_mm = rain_event_total_mm, r_e_hrs = rain_event_hrs, r_e_intMax = rain_int_mmPERmin_max, r_e_intMean = rain_int_mmPERmin_mean, 
         r_pre1d = rain_preEvent_1d, r_pre7d = rain_preEvent_7d, r_pre14d = rain_preEvent_14d, r_pre30d = rain_preEvent_30d, 
         q_e_max = q_event_max, q_e_del = q_event_delta, q_e_delQrt = q_event_dQRate_cmsPerHr, q_pre_1d = q_preEvent_mean_1d, 
         q_pre_7d = q_preEvent_mean_7d, NO3 = NO3_kg_km2, SRP = SRP_kg_km2, NO3toSRP = event_NO3_SRP, turb = turb_kg_km2, turb_max = turb_event_max,
         gw_e_max = gw_event_max_well3, gw_e_del = gw_event_delta_well3, gw_pre_1d = gw_preEvent_mean_1d_well3, gw_pre_7d = gw_preEvent_mean_7d_well3,
         TSLE = time_sinceLastEvent, PET = PET_mmHR, DO_p_up = DO_mean_preEvent_1, RED_pre_up = Redox_mean_preEvent_1, 
         ST_pre_up = SoilTemp_mean_preEvent_1, VWC_pre_up = VWC_mean_preEvent_1, DO_p_low = DO_mean_preEvent_6, RED_pre_low = Redox_mean_preEvent_6,
         ST_pre_low = SoilTemp_mean_preEvent_6, VWC_pre_low = VWC_mean_preEvent_6)
```

Variable names and abbreviations
```{r, include = TRUE, warning = FALSE, echo = FALSE}
variable <- c("site", "event_start", "season", "rain_event_total_mm", "rain_event_hrs", "rain_int_mmPERmin_max", "rain_int_mmPERmin_mean", "rain_preEvent_1d", 
              "rain_preEvent_7d", "rain_preEvent_14d", "rain_preEvent_30d", "q_event_max", "q_event_delta", "q_event_dQRate_cmsPerHr", "q_preEvent_mean_1d",
              "q_preEvent_mean_7d", "q_mm", "NO3_kg_km2", "SRP_kg_km2", "event_NO3_SRP", "turb_kg_km2", "turb_event_max", "gw_preEvent_mean_1d_well", 
              "gw_event_delta_well", "gw_preEvent_mean_1d_well", "gw_preEvent_mean_7d_well", "time_sinceLastEvent", "multipeak", "PET_mmHR", "DO_mean_preEvent_", 
              "Redox_mean_preEvent_", "SoilTemp_mean_preEvent_", "VWC_mean_preEvent_", "DO_mean_preEvent_", "Redox_mean_preEvent_", "SoilTemp_mean_preEvent_", 
              "VWC_mean_preEvent_")
abbrev <- c("site", "event_start", "season", "r_e_mm", "r_e_hrs", "r_e_intMax", "r_e_intMean", "r_pre1d", 
              "r_pre7d", "r_pre14d", "r_pre30d", "q_e_max", "q_e_del", "q_e_delQrt", "q_pre_1d", "q_pre_7d", 
              "q_mm", "NO3", "SRP", "NO3toSRP", "turb", "turb_max", "gw_e_max", "gw_e_del", "gw_pre_1d",
              "gw_pre_7d", "TSLE", "multipeak", "PET", "DO_p_up", "RED_pre_up", "ST_pre_up", "VWC_pre_up",
              "DO_p_low", "RED_pre_low", "ST_pre_low", "VWC_pre_low")
var_abbrevs <- data.frame(variable, abbrev)
var_abbrevs
```

```{r, include = TRUE, warning = FALSE}
# Create df's with and without response variables (i.e., NO3_kg_km2, SRP_kg_km2, event_NO3_SRP)
# Site: Hungerford Brook
hf_with <- hford %>% 
  select_if(is.numeric) %>% 
  # Use complete cases only
  na.omit()
hf_without <- hf_with %>% 
  select(-c(NO3, SRP, NO3toSRP))
# Site: Wade Brook
wd_with <- wade %>% 
  select_if(is.numeric) %>% 
  na.omit()
wd_without <- wd_with %>% 
  select(-c(NO3, SRP, NO3toSRP))

# Dimensions of df's
# Hungerford
dim(hf_with)
# Wade
dim(wd_with)
```
<br><br>
Should we transform the data before doing PCA?
```{r, include = TRUE, warning = FALSE, fig.width = 7, fig.height = 7}
# Look at histograms of data
hf_with %>% 
  # Gather all columns into one long column
  pivot_longer(cols = r_e_mm:VWC_pre_low, names_to = "variable", values_to = "value") %>% 
  # Plot of histograms
  ggplot(aes(x = value)) +
  facet_wrap(~variable, scales = "free", ncol = 5) + 
  geom_histogram() +
  theme(strip.text = element_text(size = 7))
# Let's log transform all variables for now
hf_with %>% 
  # Here I transform the date: log(value + 500)
  mutate_all(list(~ log(. + 500))) %>% 
  # Gather all columns into on long column
  pivot_longer(cols = r_e_mm:VWC_pre_low, names_to = "variable", values_to = "value") %>% 
  # Plot of histograms
  ggplot(aes(x = value)) +
  facet_wrap(~variable, scales = "free", ncol = 5) + 
  geom_histogram() +
  xlab("log(value + 500") +
  theme(strip.text = element_text(size = 7))
# That was not really effective, let's try using the transformTukey function from the rcompanion package
# https://rcompanion.org/handbook/I_12.html
hf_with %>% 
  # Apply the transformTukey function to all variables
  mutate_all(list(~ transformTukey(. + 500, plotit = FALSE, quiet = TRUE))) %>% 
  pivot_longer(cols = r_e_mm:VWC_pre_low, names_to = "variable", values_to = "value") %>% 
  ggplot(aes(x = value)) +
  facet_wrap(~variable, scales = "free", ncol = 5) + 
  geom_histogram() +
  xlab("transformTukey(value + 500)") +
  theme(strip.text = element_text(size = 7))
# Create dfs w/ transformed data as an option
hf_with_trans <- hf_with %>% 
  # Apply the transformTukey function to all variables
  mutate_all(list(~ transformTukey(. + 500, plotit = FALSE, quiet = TRUE)))  
hf_without_trans <- hf_without %>% 
  mutate_all(list(~ transformTukey(. + 500, plotit = FALSE, quiet = TRUE))) 
wd_with_trans <- wd_with %>% 
  mutate_all(list(~ transformTukey(. + 500, plotit = FALSE, quiet = TRUE))) 
wd_without_trans <- wd_without %>% 
  mutate_all(list(~ transformTukey(. + 500, plotit = FALSE, quiet = TRUE))) 
```
<br><br>
I tried using feature importance in random forests to reduce the number of variables for the SOM, but it wasn't very effective. My question then is: how do I use PCA to reduce the number of variables/dimensionality? <br />
Use 1: Do I do PCA on all variables including response variables (NO3 & SRP yield & NO3 to SPR yield ratios) and somehow select variables based on how they map out in PCA space? <br />
Use 2: Or do I keep all predictor variables and then do PCA on them to create orthogonal variables and reduce dimensionality of the dataset such as in Pearce et al. 2011? <br><br>

PCA use 1: examining how all variables map out in PCA space <br />
I'm also unsure exactly how to do PCA properly, so I include multiple options below
```{r, include = FALSE}
# Helpful webistes
  # https://aaronschlegel.me/principal-component-analysis-r-example.html
  # http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/
  # https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance
  # https://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont
```

```{r, include = FALSE}
pcaCharts <- function(x) {
    vars <- apply(x$x, 2, var)
    props <- vars / sum(vars)
    df <- props %>% 
      data.frame(as.list(.)) %>% 
      select(prop_var = ".") %>% 
      as_tibble(rownames = "PC") %>% 
      mutate(PC_num = parse_number(PC)) %>% 
      mutate(prop_var_cum = cumsum(prop_var))
    
    p1 <- ggplot(data = df, aes(x = PC_num, y = prop_var)) +
      geom_point() +
      ylab("Prop. of var.") +
      theme_classic() + 
      theme(axis.title.x = element_blank())
    p2 <- ggplot(data = df, aes(x = PC_num, y = prop_var_cum)) +
      geom_point() +
      xlab("PC") +
      ylab("Cumm. prop. of var.") +
      theme_classic()
    p3 <- plot_grid(p1, p2, nrow = 2, align = "hv")
    p4 <- factoextra::fviz_pca_var(x, repel = TRUE)
    p5 <- plot_grid(p3, p4, nrow = 1, align = "v", rel_widths = c(1.5, 2))
    theme1 <- theme_classic() +
      theme(plot.title = element_text(size = 9))
    p6 <- factoextra::fviz_contrib(x, choice = "var", ggtheme = theme1, axes = 1, top = 10)
    p7 <- factoextra::fviz_contrib(x, choice = "var", ggtheme = theme1, axes = 2, top = 10)
    p8 <- factoextra::fviz_contrib(x, choice = "var", ggtheme = theme1, axes = 3, top = 10)
    p9 <- factoextra::fviz_contrib(x, choice = "var", ggtheme = theme1, axes = 4, top = 10)
    p10 <- plot_grid(p6, p7, p8, p9, nrow = 1)
    plot_grid(p5, p10, nrow = 2, rel_heights = c(2, 1))
}
```

```{r, include = TRUE, warning = FALSE, fig.width = 10, fig.height = 7}
# SITE = HUNGERFORD BROOK
# Including predictors (NO3 & SRP yield & NO3:SRP yield ratio); variables are NOT transformed to improve normality
# PCA on the Pearson CORRELATION matrix
# This (using the corr. matrix) seems like the proper method given untransformed (raw) data 
# where variables have different units and ranges
  PC_hf_with_cor <- prcomp(hf_with, scale = TRUE)
  pcaCharts(PC_hf_with_cor)
# PCA on the COVARIANCE matrix
  PC_hf_with_cov <- prcomp(hf_with, scale = FALSE)
  pcaCharts(PC_hf_with_cov)
# Including precitors; variables ARE transformed to improve normality
# PCA on the Pearson CORRELATION matrix
  PC_hf_with_trans_cor <- prcomp(hf_with_trans, scale = TRUE)
  pcaCharts(PC_hf_with_trans_cor)
# PCA on the COVARIANCE matrix
  PC_hf_with_trans_cov <- prcomp(hf_with_trans, scale = FALSE)
  pcaCharts(PC_hf_with_trans_cov)
```
I think I should probably do PCA with the correlation matrix. <br /> 
Visually the results of the PCA with untransformed vs. transformed distributions look fairly similar, though not sure how to test. Maybe stick with untransformed distributions b/c easier to explain? <br /> 
I think prcomp uses the Pearson correlation coefficient. In Pearce et al. they use the Spearman rank correlation matrix. I don't think you can implement this using `prcomp()`, but you can do it using `princomp()`. According to this website: http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/ `prcomp()` is preferred b/c it uses the singular value decomposition (SVD) vs. the spectral decomposition approach, the former having greater numerical accuracy. But! `princomp()` allows you to input whatever correlation matrix you want! Let's try it.
```{r, fig.width = 8, fig.height = 4}
corr_spearman <- cor(hf_with, method = "spearman")
PC_hf_with_cor_sp <- princomp(covmat = corr_spearman, scores = TRUE)
# get_eig(PC_hf_with_cor_sp)
# summary(PC_hf_with_cor_sp)
p1 <- factoextra::fviz_eig(PC_hf_with_cor_sp)
p2 <- factoextra::fviz_pca_var(PC_hf_with_cor_sp, repel = TRUE)
plot_grid(p1, p2, nrow = 1, rel_widths = c(1,2))
```
The results look fairly similar, except that the PCA plot is inverted. Maybe stick with `prcomp()` because it is supposed to be more numerically accurate?
<br><br>

PCA use 2: create orthogonal variables and reduce dimensionality of the dataset (response variables not included) <br />
```{r, include = TRUE, warning = FALSE, fig.width = 10, fig.height = 7}
# PCA on the Pearson CORRELATION matrix; variables are NOT transformed to improve normality
  PC_hf_without_cor <- prcomp(hf_without, scale = TRUE)
  pcaCharts(PC_hf_without_cor)
# PCA on the Pearson CORRELATION matrix; variables ARE transformed to improve normality
  PC_hf_without_trans_cor <- prcomp(hf_without_trans, scale = TRUE)
  pcaCharts(PC_hf_without_trans_cor)
```
Q1: Using untransformed vs. transformed distributions results in slightly different results (visually). Which to use and how to figure that out? <br />
Q2: How to know how many PCs are unique/how many PCs should I keep? <br><br>

Let's move forward with PC_hf_without_cor (Hford; without predictors; Pearson correlation matrix on untransformed distributions) <br />
Extract and normalize b/w 0 and 1 like in Pearce et al. 2011
```{r, include = TRUE, warning = FALSE}
# Output scores from PCA
hf_scores <- as_tibble(PC_hf_without_cor$x, rownames = "row_number") %>% mutate(row_number = as.integer(row_number))
# Join scores to original data & normalize each PC b/w 0 and 1
hf_PCAresuls <- full_join(hford %>% na.omit() %>% mutate(row_number = seq.int(nrow(.))),
                          hf_scores,
                          by = "row_number") %>% 
  # And normalize each PC between 0 and 1
  mutate_at(vars(c(PC1:ncol(.))),
            .funs = list(~ (. - min(.)) / (max(.) - min(.))))
```
And then on to SOM? Thoughts?